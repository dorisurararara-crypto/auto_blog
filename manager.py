import os
import time
import sqlite3
import re
from datetime import datetime
from dotenv import load_dotenv

from src.collector.reddit_collector import RedditCollector
from src.collector.google_searcher import GoogleSearcher
from src.processor.gemini_analyzer import GeminiAnalyzer
from src.processor.claude_processor import ClaudeProcessor
from src.painter.local_painter import LocalPainter
from src.affiliate.coupang_helper import CoupangHelper

load_dotenv()

class GTBManager:
    def __init__(self):
        self.db_path = "data/gtb_storage.db"
        self._init_db()
        print("[*] GTB ë§¤ê±°ì§„ 'ë³¸ì§ˆ ê°•í™”' ì—”ì§„ ê°€ë™ ì¤‘...")
        self.collector = RedditCollector()
        self.searcher = GoogleSearcher()
        self.analyzer = GeminiAnalyzer()
        self.processor = ClaudeProcessor()
        self.painter = LocalPainter()
        self.affiliate = CoupangHelper()
        
        self.category_map = {
            "Supplements": "ê±´ê°•",
            "Gadgets": "ITí…Œí¬",
            "HomeImprovement": "ë¼ì´í”„",
            "Technology": "ITí…Œí¬",
            "BuyItForLife": "ë¼ì´í”„",
            "LifeProTips": "ê¿€íŒ",
            "ExplainLikeImFive": "ì§€ì‹",
            "Biohacking": "ê±´ê°•"
        }
        self.target_subreddits = list(self.category_map.keys())

    def _extract_search_query(self, title):
        """ì œëª©ì—ì„œ ê²€ìƒ‰ì— ìœ ë¦¬í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ë¶ˆìš©ì–´ ì œê±° (ê°„ì´ ë²„ì „)
        stop_words = ['how', 'to', 'the', 'a', 'is', 'are', 'what', 'why', 'in', 'of', 'for', 'on', 'with', 'and', 'my', 'do', 'any', 'best', 'new']
        words = re.sub(r'[^a-zA-Z\s]', '', title).lower().split()
        filtered_words = [w for w in words if w not in stop_words and len(w) > 2]
        
        # í•µì‹¬ í‚¤ì›Œë“œ 3-4ê°œ ì¡°í•©
        return " ".join(filtered_words[:4])

    def _init_db(self):
        os.makedirs("data", exist_ok=True)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("CREATE TABLE IF NOT EXISTS posts (reddit_id TEXT PRIMARY KEY, title TEXT, processed_date TEXT, file_path TEXT)")
        conn.commit()
        conn.close()

    def is_already_processed(self, reddit_id):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM posts WHERE reddit_id = ?", (reddit_id,))
        result = cursor.fetchone()
        conn.close()
        return result is not None

    def mark_as_processed(self, reddit_id, title, file_path):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("INSERT OR IGNORE INTO posts (reddit_id, title, processed_date, file_path) VALUES (?, ?, ?, ?)",
            (reddit_id, title, datetime.now().strftime("%Y-%m-%d %H:%M:%S"), file_path))
        conn.commit()
        conn.close()

    def sanitize_filename(self, filename):
        filename = re.sub(r'[\/:*?"<>|]', '', filename)
        filename = filename.replace(' ', '_')
        return filename[:50]

    def parse_claude_result(self, raw_text):
        data = {}
        patterns = {
            'vs_title': r'VS_TITLE:\s*(.*?)(?:\n---|\nTITLE:|$)',
            'title': r'TITLE:\s*(.*?)(?:\n---|\nSUMMARY:|$)',
            'summary': r'SUMMARY:\s*(.*?)(?:\n---|\nCONTENT:|$)',
            'content': r'CONTENT:\s*(.*?)(?:\n---|\nIMAGE_PROMPT:|$)',
            'image_prompt': r'IMAGE_PROMPT:\s*(.*?)(?:\n---|\nKEYWORDS:|$)',
            'keywords': r'KEYWORDS:\s*(.*)'
        }
        for key, pattern in patterns.items():
            match = re.search(pattern, raw_text, re.DOTALL | re.IGNORECASE)
            if match:
                data[key] = match.group(1).strip()
            else:
                data[key] = ""
        if not data.get('keywords') and data.get('title'):
            data['keywords'] = " ".join(data['title'].split()[:2])
        return data

    def run_pipeline(self):
        print("\n" + "="*60)
        print(f"ğŸš€ GTB ìˆ˜ìµí™”/ìœ ì… ìµœì í™” ëª¨ë“œ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)

        today_str = datetime.now().strftime("%Y%m%d")

        for sub in self.target_subreddits:
            # í›„ë³´êµ° 10ê°œë¥¼ ê°€ì ¸ì˜´ (Geminiê°€ ë¶„ì„í•  ì¬ë£Œ)
            posts = self.collector.fetch_top_posts(sub, limit=10)
            category_name = self.category_map.get(sub, "ì¸ì‚¬ì´íŠ¸")
            
            # ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ í›„ë³´ë“¤ë§Œ ì„ ë³„
            candidates = [p for p in posts if not self.is_already_processed(p['id'])]
            
            if not candidates:
                print(f"[-] {sub} ì¹´í…Œê³ ë¦¬ì— ìƒˆë¡œìš´ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            # [í•µì‹¬] Geminiê°€ ìˆ˜ìµì„±ê³¼ ìœ ì… í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì£¼ì œ 1ê°œë§Œ ì„ ì •
            print(f"[*] Geminiê°€ {len(candidates)}ê°œì˜ í›„ë³´ ì¤‘ 'í™©ê¸ˆ ì£¼ì œ' ë¶„ì„ ì¤‘...")
            selected_posts = self.analyzer.analyze_and_rank_topics(candidates)
            
            published_in_sub = False
            for post in selected_posts:
                print(f"[!] ìµœì¢… ë‹¹ì²¨! ({sub}): {post['title']}")
                
                # êµ¬ì²´ì ì¸ ë¡±í…Œì¼ í‚¤ì›Œë“œë¡œ í•œêµ­ íŠ¸ë Œë“œ ê²€ìƒ‰
                search_query = post.get('target_keywords', post['title']).split(',')[0]
                korean_trends = self.searcher.search_korean_trends(search_query)
                
                # ê³ ë„í™”ëœ Claude í”„ë¡œì„¸ì„œë¡œ ê¸€ ìƒì„±
                processed_text = self.processor.process_post(post, korean_trends=korean_trends)
                if not processed_text: continue
                parsed_data = self.parse_claude_result(processed_text)
                
                img_prompt = parsed_data.get('image_prompt', "Professional photography")
                image_filename = f"thumb_{post['id']}.png"
                self.painter.generate_image(img_prompt, image_filename)
                
                os.makedirs("public/images", exist_ok=True)
                if os.path.exists(f"data/images/{image_filename}"):
                    import shutil
                    shutil.move(f"data/images/{image_filename}", f"public/images/{image_filename}")
                
                keywords_raw = parsed_data.get('keywords', "").replace("[", "").replace("]", "").split(",")
                search_keyword = "ì¸ê¸°ìƒí’ˆ"
                for kw in keywords_raw:
                    clean_kw = kw.strip()
                    if clean_kw and len(clean_kw) > 1:
                        search_keyword = clean_kw
                        break
                
                coupang_items = self.affiliate.search_products(search_keyword, limit=3)
                if not coupang_items:
                    fallback_kw = " ".join(parsed_data.get('title', '').split()[:2])
                    coupang_items = self.affiliate.search_products(fallback_kw, limit=3)

                # [DB ì €ì¥ ë¡œì§] Cloudflare D1ì— ì§ì ‘ INSERT
                safe_title = parsed_data.get('title', 'no_title').replace("'", "''")
                safe_summary = parsed_data.get('summary', '').replace("'", "''")
                # ë³¸ë¬¸ ë§ˆí¬ë‹¤ìš´ ê²°í•© (ìˆ˜ìµí™” CTA ë° ë²„íŠ¼ ê°•í™”)
                full_content = f"## ğŸ’¡ í•µì‹¬ ìš”ì•½\n{parsed_data.get('summary')}\n\n{parsed_data.get('content')}"
                if coupang_items:
                    full_content += "\n\n---\n### ğŸ›’ ì¶”ì²œ ìƒí’ˆ (ìµœì €ê°€ ë° ì¬ê³  í™•ì¸)\n"
                    for item in coupang_items:
                        full_content += f"- **[{item['name']}]({item['link']})** ({item['price']}ì›) - *ì‹¤ì‹œê°„ í• ì¸ í™•ì¸í•˜ê¸°*\n"
                    full_content += "\n*ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ í™œë™ì˜ ì¼í™˜ìœ¼ë¡œ ìˆ˜ìˆ˜ë£Œë¥¼ ì œê³µë°›ìŠµë‹ˆë‹¤.*\n"
                
                safe_content = full_content.replace("'", "''")
                slug = f"{today_str}-{post['id']}"
                image_url = f"/images/{image_filename}"
                
                print(f"[*] DBì— í¬ìŠ¤íŒ… ì €ì¥ ì¤‘: {safe_title}")
                
                # D1 ì‹¤í–‰
                db_name = "auto-blog-db"
                sql = f"INSERT OR IGNORE INTO posts (slug, title, summary, content, category, image_url) VALUES ('{slug}', '{safe_title}', '{safe_summary}', '{safe_content}', '{category_name}', '{image_url}');"
                
                with open("temp.sql", "w", encoding="utf-8") as f:
                    f.write(sql)
                
                os.system(f"npx wrangler d1 execute {db_name} --remote --file=temp.sql --yes")
                os.remove("temp.sql")

                self.mark_as_processed(post['id'], parsed_data.get('title'), f"db://{slug}")
                print(f"[+++] DB ë°œí–‰ ì™„ë£Œ: {slug}")
                
                os.system("git add public/images/*")
                os.system(f"git commit -m \"Image: {image_filename}\"")
                os.system("git push origin main")
                
                published_in_sub = True
                time.sleep(5)
                break 

            if not published_in_sub:
                print(f"[-] {sub} ì¹´í…Œê³ ë¦¬ì— ìƒˆë¡œ ë°œí–‰í•  ìˆ˜ ìˆëŠ” ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

        print("\n" + "="*60)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ.")
        print("="*60)

if __name__ == "__main__":
    manager = GTBManager()
    manager.run_pipeline()