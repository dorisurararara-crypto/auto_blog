import os
import time
import sqlite3
import re
from datetime import datetime
from dotenv import load_dotenv

from src.collector.reddit_collector import RedditCollector
from src.collector.google_searcher import GoogleSearcher
from src.processor.claude_processor import ClaudeProcessor
from src.painter.local_painter import LocalPainter
from src.affiliate.coupang_helper import CoupangHelper

load_dotenv()

class GTBManager:
    def __init__(self):
        self.db_path = "data/gtb_storage.db"
        self._init_db()
        print("[*] GTB ë§¤ê±°ì§„ ì—”ì§„ ìµœì í™” ë²„ì „ ê°€ë™ ì¤‘...")
        self.collector = RedditCollector()
        self.searcher = GoogleSearcher()
        self.processor = ClaudeProcessor()
        self.painter = LocalPainter()
        self.affiliate = CoupangHelper()
        
        self.category_map = {
            "Supplements": "ê±´ê°•",
            "Gadgets": "ITí…Œí¬",
            "HomeImprovement": "ë¼ì´í”„",
            "Technology": "ITí…Œí¬",
            "BuyItForLife": "ë¼ì´í”„"
        }
        self.target_subreddits = list(self.category_map.keys())

    def _init_db(self):
        os.makedirs("data", exist_ok=True)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("CREATE TABLE IF NOT EXISTS posts (reddit_id TEXT PRIMARY KEY, title TEXT, processed_date TEXT, file_path TEXT)")
        conn.commit()
        conn.close()

    def is_already_processed(self, reddit_id):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM posts WHERE reddit_id = ?", (reddit_id,))
        result = cursor.fetchone()
        conn.close()
        return result is not None

    def mark_as_processed(self, reddit_id, title, file_path):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("INSERT INTO posts (reddit_id, title, processed_date, file_path) VALUES (?, ?, ?, ?)",
            (reddit_id, title, datetime.now().strftime("%Y-%m-%d %H:%M:%S"), file_path))
        conn.commit()
        conn.close()

    def sanitize_filename(self, filename):
        filename = re.sub(r'[\/:*?"<>|]', '', filename)
        filename = filename.replace(' ', '_')
        return filename[:50]

    def parse_claude_result(self, raw_text):
        """ê°•í™”ëœ íŒŒì‹± ë¡œì§: í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ìœ ì—°í•˜ê²Œ ëŒ€ì‘"""
        data = {}
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê° ì„¹ì…˜ ì¶”ì¶œ
        patterns = {
            'title': r'TITLE:\s*(.*?)(?:\n---|\nSUMMARY:|$)',
            'summary': r'SUMMARY:\s*(.*?)(?:\n---|\nCONTENT:|$)',
            'content': r'CONTENT:\s*(.*?)(?:\n---|\nIMAGE_PROMPT:|$)',
            'image_prompt': r'IMAGE_PROMPT:\s*(.*?)(?:\n---|\nKEYWORDS:|$)',
            'keywords': r'KEYWORDS:\s*(.*)'
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, raw_text, re.DOTALL | re.IGNORECASE)
            if match:
                data[key] = match.group(1).strip()
            else:
                data[key] = ""
        
        # ë§Œì•½ keywordsê°€ ë¹„ì–´ìˆë‹¤ë©´ ì œëª©ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not data.get('keywords') and data.get('title'):
            # ì œëª©ì˜ ì²« ë‘ ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
            data['keywords'] = " ".join(data['title'].split()[:2])
            
        return data

    def run_pipeline(self):
        print("\n" + "="*60)
        print(f"ğŸš€ GTB ìë™ í¬ìŠ¤íŒ… ì‹œì‘ (ì¿ íŒ¡ ë§í¬ ê°•í™”): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)

        today_str = datetime.now().strftime("%Y%m%d")

        for sub in self.target_subreddits:
            posts = self.collector.fetch_top_posts(sub, limit=1)
            category_name = self.category_map.get(sub, "ì¸ì‚¬ì´íŠ¸")
            
            for post in posts:
                if self.is_already_processed(post['id']):
                    continue

                search_query = " ".join(post['title'].split()[:3])
                korean_trends = self.searcher.search_korean_trends(search_query)
                processed_text = self.processor.process_post(post, korean_trends=korean_trends)
                if not processed_text: continue
                parsed_data = self.parse_claude_result(processed_text)
                
                # ì´ë¯¸ì§€ ìƒì„± ë° ì´ë™
                img_prompt = parsed_data.get('image_prompt', "Professional photography")
                image_filename = f"thumb_{post['id']}.png"
                self.painter.generate_image(img_prompt, image_filename)
                os.makedirs("public/images", exist_ok=True)
                if os.path.exists(f"data/images/{image_filename}"):
                    os.rename(f"data/images/{image_filename}", f"public/images/{image_filename}")
                
                # ì¿ íŒ¡ ìƒí’ˆ ê²€ìƒ‰ (í‚¤ì›Œë“œ ì •ì œ ë¡œì§ ì¶”ê°€)
                print(f"[*] ì¿ íŒ¡ ìƒí’ˆ ê²€ìƒ‰ ì‹œë„ (í‚¤ì›Œë“œ: {parsed_data.get('keywords')})")
                keywords_raw = parsed_data.get('keywords', "").replace("[", "").replace("]", "").split(",")
                # ì²« ë²ˆì§¸ ìœ íš¨í•œ í‚¤ì›Œë“œ ì„ íƒ
                search_keyword = "ì¸ê¸°ìƒí’ˆ"
                for kw in keywords_raw:
                    clean_kw = kw.strip()
                    if clean_kw and len(clean_kw) > 1:
                        search_keyword = clean_kw
                        break
                
                coupang_items = self.affiliate.search_products(search_keyword, limit=3)
                
                # ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì œëª©ì—ì„œ í•œ ë²ˆ ë” ì‹œë„
                if not coupang_items:
                    fallback_kw = " ".join(parsed_data.get('title', '').split()[:2])
                    print(f"[*] ê²°ê³¼ ì—†ìŒ. ëŒ€ì²´ í‚¤ì›Œë“œë¡œ ì¬ê²€ìƒ‰: {fallback_kw}")
                    coupang_items = self.affiliate.search_products(fallback_kw, limit=3)

                # íŒŒì¼ ì €ì¥
                safe_title = self.sanitize_filename(parsed_data.get('title', 'no_title'))
                final_filename = f"{today_str}_{safe_title}.md"
                final_post_path = f"src/content/blog/{final_filename}"
                os.makedirs("src/content/blog", exist_ok=True)
                
                with open(final_post_path, "w", encoding="utf-8") as f:
                    f.write("---\n")
                    f.write(f"title: \"{parsed_data.get('title')}\"\n")
                    f.write(f"summary: \"{parsed_data.get('summary')}\"\n")
                    f.write(f"image: \"/images/{image_filename}\"\n")
                    f.write(f"category: \"{category_name}\"\n")
                    f.write("---\n\n")
                    
                    f.write(f"## ğŸ’¡ í•µì‹¬ ìš”ì•½\n{parsed_data.get('summary')}\n\n")
                    f.write(f"{parsed_data.get('content')}\n\n")
                    
                    if coupang_items:
                        f.write("\n---\n### ğŸ›’ ì¶”ì²œ ì•„ì´í…œ\n")
                        for item in coupang_items:
                            f.write(f"- **[{item['name']}]({item['link']})** ({item['price']}ì›)\n")
                        f.write("\n\n*ì´ í¬ìŠ¤íŒ…ì€ ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ í™œë™ì˜ ì¼í™˜ìœ¼ë¡œ ìˆ˜ìˆ˜ë£Œë¥¼ ì œê³µë°›ìŠµë‹ˆë‹¤.*\n")
                    else:
                        print("[!] ìµœì¢…ì ìœ¼ë¡œ ì¿ íŒ¡ ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                self.mark_as_processed(post['id'], parsed_data.get('title'), final_post_path)
                print(f"[+++] ë°œí–‰ ì™„ë£Œ: {final_post_path}")
                
                os.system("git add .")
                os.system(f"git commit -m \"Post: {parsed_data.get('title')}\"")
                os.system("git push origin main")
                time.sleep(5)

        print("\n" + "="*60)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ.")
        print("="*60)

if __name__ == "__main__":
    manager = GTBManager()
    manager.run_pipeline()