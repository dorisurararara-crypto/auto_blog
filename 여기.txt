import os
import traceback
import re
import json
import random
import requests
from urllib.parse import quote
from bs4 import BeautifulSoup
from mutagen.mp3 import MP3
import hmac
import hashlib
import time
import base64
from moviepy.editor import AudioFileClip
# --- ë²¤ë” ë¼ì´ë¸ŒëŸ¬ë¦¬ (pip install í•„ìš”) ---
from openai import OpenAI
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# from google.cloud import texttospeech # ì‚¬ìš© ì•ˆ í•¨
from elevenlabs import ElevenLabs
from moviepy.editor import *
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import pickle
import subprocess # ë§¨ ìœ„ì— ì´ importê°€ ì—†ë‹¤ë©´ ì¶”ê°€í•´ì£¼ì„¸ìš”

FFMPEG_EXE = r"C:\Users\pc\AppData\Local\Microsoft\WinGet\Packages\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\ffmpeg-8.0.1-full_build\bin\ffmpeg.exe"

os.environ["IMAGEIO_FFMPEG_EXE"] = FFMPEG_EXE  # imageio/moviepyê°€ ffmpeg ì°¾ì„ ë•Œ ì‚¬ìš©

from moviepy.config import change_settings
change_settings({"FFMPEG_BINARY": FFMPEG_EXE})  # moviepy ìì²´ ffmpeg ê³ ì •

# ==============================================================================
# --- âš™ï¸ 0ë‹¨ê³„: ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ ---
# ==============================================================================
# --- íŒŒì¼ ì„¤ì • ---
API_KEY_FILE = "api_key.txt"
GOOGLE_CREDENTIALS_FILE = "google_credentials.txt"
PEXELS_API_KEY_FILE = "pexels_api_key.txt"
ELEVEN_API_KEY_FILE = "eleven_api_key.txt" 
BGM_FILE = "bgm.mp3"
FONT_FILE = "GmarketSansTTFBold.ttf"
USED_ARTICLES_FILE = "used_articles.json"  # ì´ë¯¸ ì˜ìƒ ë§Œë“  ê¸°ì‚¬ URL ê¸°ë¡
EFFECT_TRIGGER_TEXT = "ë‚´ìš©ì´ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ ì•„ë˜ í™©ê¸ˆì—´ì‡ ë¥¼ ë¹ ë¥´ê²Œ ë‘ë²ˆ ëˆŒëŸ¬ì£¼ì„¸ìš”"
EFFECT_SOUND_FILE = "dingdong.mp3"   # ê°™ì€ í´ë”ì— ëµë™ íš¨ê³¼ìŒ íŒŒì¼ ë„£ì–´ë‘ê¸°
EFFECT_SOUND_FILE2 = "countdown.mp3" 
TALISMAN_VIDEO_FILE = "talisman_key.mp4" 
INTRO_SOUND_FILE = "intro_effect.mp3"

# --- ElevenLabs ì„¤ì • ---
# í•œêµ­ì–´ì— ì í•©í•œ ë³´ì´ìŠ¤ ID (ë‚¨ì„±/ì—¬ì„± ì¤‘ ì„ íƒ, ì—¬ê¸°ì„  ê¸°ë³¸ê°’ìœ¼ë¡œ í•˜ë‚˜ ì§€ì •)
# ì˜ˆ: "JBFqnCBsd6RMkjVDRZzb" (George - êµµì€ ë‚¨ì„±í†¤), "CwhRBWXzGAHq8TQ4Fs17" (Roger - ì°¨ë¶„í•œ í†¤)
# ì–´ë¥´ì‹  íƒ€ê²Ÿì´ë¯€ë¡œ ë˜ë ·í•˜ê³  ì‹ ë¢°ê° ìˆëŠ” ëª©ì†Œë¦¬ ì¶”ì²œ
ELEVENLABS_VOICE_ID = "U1cJYS4EdbaHmfR7YzHd" 
ELEVEN_MODEL_ID = "eleven_multilingual_v2"


#--ì¿ íŒ¡í•¨ìˆ˜--
COUPANG_CONFIG_FILE = "coupang_config.json"
LINKTREE_ADMIN_API = "https://my-linktree-app.dorisurararara.workers.dev/api/admin/links"
API_SECRET ="s3jf90dksl23kf09wjekf9023"
# --- ìŠ¤í¬ë˜í•‘ ì„¤ì • ---
HEALTH_CHOSUN_NEWS_LIST_URL = "https://m.health.chosun.com/article_list/article_list.html"
SENIOR_KEYWORDS = ['ì¹˜ë§¤', 'ê´€ì ˆì—¼', 'ë‹¹ë‡¨', 'í˜ˆì••', 'ë°±ë‚´ì¥', 'ë‚™ìƒ', 'ê³¨ì ˆ', 'ëŒ€ìƒí¬ì§„', 'ë…¸ë…„', 'ë…¸í›„', '50ëŒ€', '60ëŒ€', '70ëŒ€', 'ê±´ê°•', 'ë¶€ëª¨ë‹˜', 'ê°±ë…„ê¸°', 'ì˜ˆë°©']
ARTICLE_LIMIT = 10
ENABLE_YOUTUBE_UPLOAD = True # âœ… ìœ íŠœë¸Œ ì—…ë¡œë“œ ì—¬ë¶€ (True: ì—…ë¡œë“œ, False: ì˜ìƒë§Œ ì œì‘)
TEST_MODE_SCRIPT_ONLY = False # âœ… ëŒ€ë³¸ ìƒì„±ê¹Œì§€ë§Œ í•˜ê³  ì¢…ë£Œ (True: ëŒ€ë³¸ë§Œ, False: ì˜ìƒê¹Œì§€ ì œì‘)

# --- ì˜ìƒ ì œì‘ ì„¤ì • ---
OUTPUT_DIR = "temp_output"
OUTPUT_FILE = "final_shorts_karaoke.mp4"
SHORTS_WIDTH = 1080
SHORTS_HEIGHT = 1920
# ğŸ”¹ ìƒˆë¡œ ì¶”ê°€: ìœ„/ì•„ë˜ ë°” ì˜ì—­ ë†’ì´
# ìƒë‹¨ì€ ìœ ì§€í•˜ê±°ë‚˜ ì¡°ê¸ˆ ì¤„ì´ê³ , í•˜ë‹¨ì„ ëŒ€í­ ëŠ˜ë ¤ ìë§‰ ê³µê°„ì„ í™•ë³´í•©ë‹ˆë‹¤.
TOP_BAR_HEIGHT = 500        
BOTTOM_BAR_HEIGHT = 650    # <-- ì—¬ê¸°ë¥¼ ëŠ˜ë¦¬ë©´ ì˜ìƒì´ ìœ„ë¡œ ì˜¬ë¼ê°€ê³  ì•„ë˜ ê²€ì€ ê³µê°„ì´ ë„“ì–´ì§‘ë‹ˆë‹¤.
VIDEO_AREA_HEIGHT = SHORTS_HEIGHT - TOP_BAR_HEIGHT - BOTTOM_BAR_HEIGHT

# âœ… ImageMagick ì‹¤í–‰íŒŒì¼ ê²½ë¡œ (í´ë” ë§ê³  magick.exeê¹Œì§€!)
IMAGEMAGICK_EXE = r"C:\Program Files\ImageMagick-7.1.2-Q16-HDRI\magick.exe"

if not os.path.exists(IMAGEMAGICK_EXE):
    raise FileNotFoundError(f"magick.exeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {IMAGEMAGICK_EXE}")

# âœ… MoviePyì— ImageMagick ê²½ë¡œ ì£¼ì…
from moviepy.config import change_settings
change_settings({"IMAGEMAGICK_BINARY": IMAGEMAGICK_EXE})

# âœ… (ì„ íƒ) ì§„ì§œ ì‹¤í–‰ë˜ëŠ”ì§€ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸
subprocess.run([IMAGEMAGICK_EXE, "-version"], check=True)
print("âœ… ImageMagick ì—°ê²° OK")

def build_output_filename_from_title(title: str) -> str:
    """
    ìœ íŠœë¸Œ ì œëª©ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆê²Œ ìŠ¬ëŸ¬ê·¸ë¡œ ë³€í™˜.
    - ë¶ˆë²• ë¬¸ì ì œê±°
    - ê³µë°±ì€ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ
    - ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„
    """
    if not title:
        title = "ì‹œë‹ˆì–´_ê±´ê°•_ì •ë³´"

    # íŒŒì¼ì— ì•ˆ ë˜ëŠ” ë¬¸ì ì œê±°
    safe = re.sub(r'[\\/*?:"<>|]', '', title)
    # ê³µë°± â†’ _
    safe = re.sub(r'\s+', '_', safe.strip())

    # ë„ˆë¬´ ê¸¸ë©´ 80ì ì •ë„ë¡œ ì»·
    if len(safe) > 80:
        safe = safe[:80]

    return safe + ".mp4"

def load_used_articles():
    """
    ì´ë¯¸ ì˜ìƒìœ¼ë¡œ ë§Œë“  ê¸°ì‚¬ URL ëª©ë¡ì„ íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜´.
    ì—†ìœ¼ë©´ ë¹ˆ set ë°˜í™˜.
    """
    if not os.path.exists(USED_ARTICLES_FILE):
        return set()
    try:
        with open(USED_ARTICLES_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        # ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥ë˜ì–´ ìˆì—ˆë‹¤ê³  ê°€ì •
        return set(data)
    except Exception as e:
        print("[ê¸°ì‚¬ ê¸°ë¡] used_articles.json ì½ê¸° ì˜¤ë¥˜:", e)
        return set()


def save_used_articles(used_urls: set):
    """
    ì‚¬ìš©ëœ ê¸°ì‚¬ URL setì„ JSON íŒŒì¼ë¡œ ì €ì¥.
    """
    try:
        with open(USED_ARTICLES_FILE, "w", encoding="utf-8") as f:
            json.dump(list(used_urls), f, ensure_ascii=False, indent=2)
        print(f"[ê¸°ì‚¬ ê¸°ë¡] ì‚¬ìš©ëœ ê¸°ì‚¬ {len(used_urls)}ê°œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print("[ê¸°ì‚¬ ê¸°ë¡] used_articles.json ì €ì¥ ì˜¤ë¥˜:", e)



def load_settings():
    """ëª¨ë“  API í‚¤ì™€ GCP ì¸ì¦ ì •ë³´ë¥¼ ë¡œë“œí•˜ê³  í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    print("="*50 + "\nâš™ï¸ 0ë‹¨ê³„: ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹œì‘...\n" + "="*50)
    settings = {}
    try:
        with open(API_KEY_FILE, 'r') as f: settings['openai_api_key'] = f.read().strip()
        with open(PEXELS_API_KEY_FILE, 'r') as f: settings['pexels_api_key'] = f.read().strip()
        with open(GOOGLE_CREDENTIALS_FILE, 'r') as f:
            gcp_creds_path = f.read().strip()
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = gcp_creds_path
         # ğŸ”¹ ElevenLabs API í‚¤ ë¡œë“œ
        try:
            with open(ELEVEN_API_KEY_FILE, 'r') as f:
                settings['eleven_api_key'] = f.read().strip()
            print("  - ElevenLabs API í‚¤ ë¡œë“œ ì™„ë£Œ")
        except FileNotFoundError:
            print("  - [ì£¼ì˜] eleven_api_key.txt ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ElevenLabs TTSëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            settings['eleven_api_key'] = None

        print("  - ëª¨ë“  ì„¤ì • íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return settings
    except FileNotFoundError as e:
        print(f"  - [ì¹˜ëª…ì  ì˜¤ë¥˜] í•„ìˆ˜ ì„¤ì • íŒŒì¼ '{e.filename}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return None

# ==============================================================================
# --- ğŸ¤– 1ë‹¨ê³„: ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ ---
# ==============================================================================
def check_profitability(client: OpenAI, title: str, content: str) -> bool:
    """
    [ë¹ ë¥¸ íŒë‹¨] ì´ ê¸°ì‚¬ê°€ ëˆì´ ë˜ëŠ”ì§€(ì¿ íŒ¡ ìƒí’ˆ ë§¤ì¹­ ê°€ëŠ¥), ê·¸ë¦¬ê³  ê´‘ê³ ê°€ ì•„ë‹Œì§€ íŒë‹¨.
    Trueê°€ ë‚˜ì˜¤ë©´ ì¦‰ì‹œ ì±„íƒ.
    """
    # 1ì°¨: íŒŒì´ì¬ ë‹¨ì–´ í•„í„°ë§ (ë¹„ìš© ì ˆê°)
    bad_keywords = ["MOU", "í˜‘ì•½", "ìˆ˜ìƒ", "ì‹œìƒ", "ê°œìµœ", "ëª¨ì§‘", "ì„ ì •", "ì›ì¥", "ë³‘ì›ì¥", "ì·¨ì„", "ê¸°ë¶€"]
    if any(bk in title for bk in bad_keywords):
        print(f"    Pass (ë³‘ì› í™ë³´ ì˜ì‹¬): {title}")
        return False

    # 2ì°¨: GPT íŒë‹¨ (êµ¬ë§¤ ì „í™˜ìœ¨ ì˜ˆì¸¡ - ë¬¼ë¦¬ì  ìƒí’ˆì„± ê²€ì¦)
    prompt = f"""
    Analyze this health news article.
    Title: {title}
    Content Start: {content[:500]}

    Task: Can the problem in this article be solved by a **PHYSICAL PRODUCT** purchasable on Coupang (e.g., Supplement, Equipment, Food, Device)?

    [Rules for TRUE]
    - Specific physical pain (Joints, Back, Eyes).
    - Visible issues (Skin, Hair loss, Diet/Fat).
    - Measurable health markers (Blood sugar, Pressure, Cholesterol).
    - Nutrient deficiency (Vitamin, Protein).

    [Rules for FALSE (Strictly Reject)]
    - **Psychological/Mental issues** (Depression, Rejection sensitivity, Attitude, Stress management).
    - **Abstract/Lifestyle advice** only (e.g., "Sleep early", "Walk more", "Meditate").
    - **Social/Policy news** (Hospital events, Government rules).
    - If the solution is purely "Change your mindset" -> FALSE.

    Output ONLY "TRUE" or "FALSE".
    """
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini", # ì‹¸ê³  ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        ans = resp.choices[0].message.content.strip().upper()
        if "TRUE" in ans:
            return True
        else:
            print(f"    Pass (GPT íŒë‹¨-ìˆ˜ìµì„± ë‚®ìŒ): {title}")
            return False
    except Exception:
        return False

def find_one_profitable_article(client: OpenAI, used_urls: set):
    print("\n" + "="*50 + "\nğŸ¤– 1ë‹¨ê³„: 'ëˆ ë˜ëŠ”' ê¸°ì‚¬ 1ê°œ ë°œêµ´ ì¤‘ (ë°œê²¬ ì¦‰ì‹œ ì¢…ë£Œ)...\n" + "="*50)
    
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--log-level=3')
    
    driver = None
    try:
        service = ChromeService(executable_path=ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(HEALTH_CHOSUN_NEWS_LIST_URL)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.story_item a")))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        article_links = soup.select("div.story_item div.title a")
        
        print(f"  - ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ {len(article_links)}ê°œ ë¡œë“œ ì™„ë£Œ. íƒìƒ‰ ì‹œì‘...")

        for link in article_links:
            article_url = requests.compat.urljoin(HEALTH_CHOSUN_NEWS_LIST_URL, link.get('href', ''))
            
            # âœ… [ì†ë„ ìµœì í™”] ì´ë¯¸ ì“´ ê¸°ì‚¬ëŠ” ì—´ì–´ë³´ì§€ë„ ì•ŠìŒ
            if article_url in used_urls:
                print(f"  - [Skip] ì´ë¯¸ ì œì‘í•œ ê¸°ì‚¬")
                continue

            # ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            title, content = fetch_and_filter_article(driver, article_url)
            if not title or not content:
                continue

            # âœ… [ìˆ˜ìµì„± íŒë‹¨] í†µê³¼í•˜ë©´ ë°”ë¡œ ë¦¬í„´ (Early Exit)
            print(f"  - ê²€í†  ì¤‘: {title[:20]}...", end="")
            if check_profitability(client, title, content):
                print(" -> ğŸ¯ í•©ê²©! ì´ ê¸°ì‚¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                return [{'title': title, 'url': article_url, 'content': content}]
            
    except Exception as e:
        print(f"  - [ì˜¤ë¥˜] ìŠ¤í¬ë˜í•‘ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
    finally:
        if driver:
            driver.quit()
    
    print("\n  - [ì‹¤íŒ¨] ì“¸ë§Œí•œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return []

def fetch_and_filter_article(driver: webdriver.Chrome, url: str):
    try:
        driver.get(url)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        title = soup.find('meta', property='og:title')['content'].strip()
        content_div = soup.select_one('section.article-body')
        if not content_div: return None, None
        for ad_element in content_div.select('.advertise_box, .news_like, .copyright, .reporter_info, script, [id*="google"], [id*="ad"]'):
            ad_element.decompose()
        content_text = re.sub(r'\n{2,}', '\n', content_div.get_text(separator='\n', strip=True))
        if any(keyword in title or keyword in content_text for keyword in SENIOR_KEYWORDS):
            return title, content_text
        else:
            print(f"  - [íŒ¨ìŠ¤] í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜: {title[:30]}...")
            return None, None
    except Exception:
        return None, None
    
def score_article_for_seniors(client: OpenAI, title: str, content: str):
    """
    ê¸°ì‚¬ í•˜ë‚˜ë¥¼ 40~70ëŒ€ ì‹¤ìƒí™œì— ì–¼ë§ˆë‚˜
    'ë„ì›€ + ìœ íŠœë¸Œ ê²€ìƒ‰ ìˆ˜ìš” + í›…/ë°˜ì „/ìŠ¤í† ë¦¬ ì ì¬ë ¥'ì´ ìˆëŠ”ì§€ 1~5ì ìœ¼ë¡œ í‰ê°€.

    - ë‹¨ìˆœíˆ ê±´ê°• ì •ë³´ê°€ ì¢‹ì€ ê¸°ì‚¬ë³´ë‹¤,
      'ìœ íŠœë¸Œì—ì„œ ì‚¬ëŒë“¤ì´ ì‹¤ì œë¡œ ë§ì´ ê²€ìƒ‰í•  ê²ƒ ê°™ê³ ,
       ì²« í›…Â·ë°˜ì „Â·í•˜ëª¬ ì„œí´ ìŠ¤í† ë¦¬ë¡œ ë½‘ê¸° ì¢‹ì€ ê¸°ì‚¬'ì— ë” ë†’ì€ ì ìˆ˜.
    - íƒ„ì†Œë§¤íŠ¸, íŠ¹ì • ì¥ë¹„/ë¸Œëœë“œ, ë³‘ì› í–‰ì‚¬/í™ë³´ ìœ„ì£¼ ê¸°ì‚¬ëŠ”
      ìœ íŠœë¸Œ ê²€ìƒ‰ ìˆ˜ìš”ì™€ ìŠ¤í† ë¦¬ ì ì¬ë ¥ì´ ë‚®ë‹¤ê³  ë³´ê³  ì ìˆ˜ ì œí•œ.
    """
    prompt = f"""
ë‹¹ì‹ ì€ 40~70ëŒ€ ì‹œë‹ˆì–´ì™€ ê·¸ ìë…€ë“¤ì„ ìœ„í•œ ìœ íŠœë¸Œ ê±´ê°• ì±„ë„ 'íšŒì¶˜41'ì˜ í¸ì§‘ì¥ì…ë‹ˆë‹¤.
ì´ ì±„ë„ì€ **ì™„ì „ ìë™í™”**ë¡œ ëŒì•„ê°€ê¸° ë•Œë¬¸ì—,
'ì‚¬ëŒë“¤ì´ ê²€ìƒ‰ë„ ì•ˆ í•˜ëŠ” ì£¼ì œ' + 'ìŠ¤í† ë¦¬/ë°˜ì „ì´ ì•ˆ ë‚˜ì˜¤ëŠ” ê¸°ì‚¬'ëŠ” ì•„ì˜ˆ ì“°ë©´ ì•ˆ ë©ë‹ˆë‹¤.

ë”°ë¼ì„œ, ì•„ë˜ ê¸°ì‚¬ë¥¼ ë„¤ ê°€ì§€ ê´€ì ì—ì„œ í‰ê°€í•˜ì„¸ìš”.

1) ì‹œë‹ˆì–´ì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ê±´ê°• ì •ë³´ì¸ì§€ (usefulness_score)
2) ìœ íŠœë¸Œì—ì„œ 40~70ëŒ€ê°€ ì‹¤ì œë¡œ ë§ì´ ê²€ìƒ‰í•  ê²ƒ ê°™ì€ ì£¼ì œì¸ì§€ (search_demand_score)
3) ì²« 3ì´ˆ í›…Â·ì œëª©ìœ¼ë¡œ ë½‘ê¸° ì¢‹ì€ì§€ (hook_potential_score)
4) ë°˜ì „ + í•˜ëª¬ ì„œí´ ìŠ¤í† ë¦¬ë¡œ í’€ê¸° ì¢‹ì€ì§€ (reversal_story_score)

[ê¸°ì‚¬ ì œëª©]
{title}

[ê¸°ì‚¬ ë‚´ìš© ì¼ë¶€]
{content[:1200]}

============================================================
1. "ë„ì›€ì´ ë˜ëŠ” ê±´ê°• ì •ë³´" í‰ê°€ (usefulness_score)
============================================================

- ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ usefulness_score (1~5)ë¥¼ ì •í•˜ì„¸ìš”.

1ì :
- ë³‘ì›/ê¸°ê´€/ì œì•½ì‚¬/í•™íšŒ/ì§€ìì²´/ê¸°ì—…ì˜ **í™ë³´, ìˆ˜ìƒ ì†Œì‹, í–‰ì‚¬ ì•ˆë‚´** ìœ„ì£¼
- ì˜ˆ: â—‹â—‹ë³‘ì› ê°œì†Œì‹, MOU ì²´ê²°, ìº í˜ì¸ ê°œìµœ, ì‹œìƒì‹, í›„ì›, í˜‘ì•½, ì¤€ê³µ, í–‰ì‚¬ ì¼ì • ë“±
- ì‹œë‹ˆì–´ê°€ ë‹¹ì¥ ìƒí™œì— ì ìš©í•  ë§Œí•œ êµ¬ì²´ì ì¸ ê±´ê°• íŒì´ ê±°ì˜ ì—†ìŒ

2ì :
- í™ë³´Â·í–‰ì‚¬ ê¸°ì‚¬ì§€ë§Œ,
  ì‹œë‹ˆì–´ì—ê²Œ ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ ê±´ê°• ìˆ˜ì¹™Â·íŒì´ ê±°ì˜ ì—†ìŒ
- ë˜ëŠ” ì •ë³´ê°€ ë„ˆë¬´ ì¶”ìƒì /ì›ë¡ ì ì´ë¼ ìƒí™œì— ë°”ë¡œ ì“°ê¸° ì–´ë ¤ì›€

3ì :
- ìƒí™œìŠµê´€, ì§ˆë³‘ ì˜ˆë°©Â·ê´€ë¦¬, ì‹ë‹¨, ìš´ë™, ê²€ì‚¬ ì£¼ê¸°, ì•½Â·í•­ìƒì œ ì‚¬ìš© ì›ì¹™ ë“±
  "ì‹œë‹ˆì–´ê°€ ë‹¹ì¥ ì ìš©í•´ë³¼ ìˆ˜ ìˆëŠ” íŒ"ì´ 1~2ê°œëŠ” ë‚˜ì˜´

4ì :
- êµ¬ì²´ì ì¸ ì›ì¹™Â·ê°€ì´ë“œÂ·ì£¼ì˜ì ì´ ì—¬ëŸ¬ ê°œ ì •ë¦¬ë˜ì–´ ìˆê³ ,
  ì‹œë‹ˆì–´ê°€ ìì‹ ì˜ ìƒí™œìŠµê´€ì„ ë°”ë¡œ ë°”ê¿”ë³¼ ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì¶©ë¶„í•¨

5ì :
- ì‹œë‹ˆì–´ê°€ ê°€ì¥ ê´€ì‹¬ ë§ê³ , ìœ„í—˜ë„ë„ ë†’ì€ ì£¼ì œ(í˜ˆì••, í˜ˆë‹¹, ì‹¬ì¥, ë‡Œ, ì¹˜ë§¤, ê´€ì ˆ, í—ˆë¦¬Â·ë¬´ë¦, ìˆ˜ë©´, ì•”, ì½œë ˆìŠ¤í…Œë¡¤, ì˜ì–‘ì œ, ë‹¤ì´ì–´íŠ¸ ë“±)ì— ëŒ€í•´
  **ë§¤ìš° êµ¬ì²´ì ì¸ ìƒí™œìˆ˜ì¹™/ì²´í¬ë¦¬ìŠ¤íŠ¸/ì£¼ì˜ì **ì´ ì˜ ì •ë¦¬ë˜ì–´ ìˆìŒ

============================================================
2. ìœ íŠœë¸Œ ê²€ìƒ‰ ìˆ˜ìš” í‰ê°€ (search_demand_score)
============================================================

- ì´ ê¸°ì‚¬ê°€ ë‹¤ë£¨ëŠ” ì£¼ì œê°€,
  ìœ íŠœë¸Œì—ì„œ 40~70ëŒ€ê°€ ì§ì ‘ ê²€ìƒ‰í•´ì„œ ë³¼ ë§Œí•œ ì£¼ì œì¸ì§€ í‰ê°€í•˜ì„¸ìš”.

ì•„ë˜ëŠ” **í•œêµ­ 40~70ëŒ€ê°€ ìœ íŠœë¸Œì—ì„œ ìì£¼ ê²€ìƒ‰í•˜ëŠ” ëŒ€í‘œ ê±´ê°• í‚¤ì›Œë“œ(ì˜ˆì‹œ)**ì…ë‹ˆë‹¤.

- í˜ˆì••, ê³ í˜ˆì••, ì €í˜ˆì••
- í˜ˆë‹¹, ë‹¹ë‡¨
- ì½œë ˆìŠ¤í…Œë¡¤, ì¤‘ì„±ì§€ë°©, ì§€ë°©ê°„, ê°„ìˆ˜ì¹˜
- ì‹¬ì¥, ì‹¬ê·¼ê²½ìƒ‰, ë‡Œì¡¸ì¤‘, ë‡Œê²½ìƒ‰, ë‡Œì¶œí˜ˆ
- ì¹˜ë§¤, ê¸°ì–µë ¥, ê±´ë§ì¦
- ê´€ì ˆ, ë¬´ë¦í†µì¦, í—ˆë¦¬í†µì¦, ë””ìŠ¤í¬, ê³¨ë‹¤ê³µì¦
- ë±ƒì‚´, ì²´ì§€ë°©, ë‹¤ì´ì–´íŠ¸, ì²´ì¤‘ê°ëŸ‰
- ìˆ˜ë©´, ë¶ˆë©´ì¦, ì½”ê³¨ì´, ìˆ˜ë©´ë¬´í˜¸í¡
- í”¼ë¡œ, ê¸°ë ¥, ê°±ë…„ê¸°
- ëˆˆ, ë°±ë‚´ì¥, í™©ë°˜ë³€ì„±, ì‹œë ¥, ì•ˆêµ¬ê±´ì¡°
- ì¹˜ê³¼, ì„í”Œë€íŠ¸, ì‡ëª¸, ì¹˜ì£¼ì—¼
- ì†Œí™”, ìœ„ì—¼, ì—­ë¥˜ì„±ì‹ë„ì—¼, ìœ„ì•”, ëŒ€ì¥ì•”, íì•”, ê°„ì•”
- ë¹„íƒ€ë¯¼, ì˜¤ë©”ê°€3, ì˜ì–‘ì œ, í”„ë¡œë°”ì´ì˜¤í‹±ìŠ¤, ìœ ì‚°ê· 
- ê±·ê¸°, ê±·ê¸°ìš´ë™, ìŠ¤íŠ¸ë ˆì¹­, ìš´ë™ë²•

ìœ„ì™€ ê°™ì€ **í° í‚¤ì›Œë“œ ë°”êµ¬ë‹ˆ**ì™€ ë¹„êµí•˜ì—¬,
ê¸°ì‚¬ì˜ ì£¼ì œë¥¼ ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

search_demand_score:

1ì  (ë‚®ìŒ):
- íƒ„ì†Œë§¤íŠ¸, íŠ¹ì • ì˜ë£Œê¸°ê¸° ëª¨ë¸ëª…, íŠ¹ì • ë³‘ì› ì¥ë¹„ ë„ì…,
  íŠ¹ì • ì§€ìì²´ ìº í˜ì¸, ì¼íšŒì„± í–‰ì‚¬, ë²•Â·ì •ì±…Â·ì œë„ ì•ˆë‚´ ë“±
- ì œí’ˆ/ë¸Œëœë“œ/ì¥ë¹„ ë‹¨ì¼ ì£¼ì œ ê¸°ì‚¬ (ì˜ˆ: â—‹â—‹ íƒ„ì†Œë§¤íŠ¸, â—‹â—‹ ê³µê¸°ì²­ì •ê¸° ì‹ ì œí’ˆ ë“±)
- ìœ íŠœë¸Œì—ì„œ ì¼ë°˜ì¸ì´ ì¼ë¶€ëŸ¬ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ ê±°ì˜ ì—†ìŒ

2ì  (ë‹¤ì†Œ ë‚®ìŒ):
- íŠ¹ì • ì œí’ˆì´ë‚˜ ì¥ë¹„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ì§€ë§Œ, ê±´ê°• ê°œë…ì€ ìˆê¸´ í•¨
- ê·¸ë˜ë„ ê²€ìƒ‰í•  ë•ŒëŠ” ì œí’ˆëª…ì´ ì•„ë‹ˆë¼,
  ë” í° ê°œë…(ì˜ˆ: "ê²¨ìš¸ ë‚œë°© ê±´ê°•", "ì „ê¸°ì¥íŒ í™”ìƒ ìœ„í—˜")ìœ¼ë¡œ ê²€ìƒ‰í•  ê°€ëŠ¥ì„±ì´ í¼

3ì  (ë³´í†µ):
- í˜ˆì••, í˜ˆë‹¹, ê´€ì ˆ, í—ˆë¦¬í†µì¦, ìˆ˜ë©´, ë‹¤ì´ì–´íŠ¸ ë“±ê³¼ ê´€ë ¨ì€ ìˆì§€ë§Œ
  ì œëª©ê³¼ ë‚´ìš©ì´ ë„ˆë¬´ ì „ë¬¸ì ì´ê±°ë‚˜,
  ë„ˆë¬´ ì¢ì€ ìƒí™©ì—ë§Œ í•´ë‹¹ë˜ëŠ” ëŠë‚Œ

4ì  (ë†’ìŒ):
- 40~70ëŒ€ê°€ ì‹¤ì œë¡œ ìì£¼ ê²€ìƒ‰í•˜ëŠ” ì£¼ì œ(í˜ˆì••, í˜ˆë‹¹, ì¹˜ë§¤, ê´€ì ˆ, ìˆ˜ë©´, í—ˆë¦¬Â·ë¬´ë¦, ì•”, ì½œë ˆìŠ¤í…Œë¡¤, ì˜ì–‘ì œ, ë‹¤ì´ì–´íŠ¸ ë“±)ì— í•´ë‹¹í•˜ê³ 
  ì œëª©ë§Œ ì˜ ë½‘ìœ¼ë©´ ê²€ìƒ‰ ìœ ì…ì´ ì¶©ë¶„íˆ ì˜ˆìƒë¨

5ì  (ë§¤ìš° ë†’ìŒ):
- ìœ„ ëŒ€í‘œ í‚¤ì›Œë“œë“¤ ì¤‘ í•˜ë‚˜ë¥¼
  **ê·¸ëŒ€ë¡œ ì œëª© ì•ì— ë°•ì•„ ë„£ì–´ë„ ì´ìƒí•˜ì§€ ì•Šì„ ì •ë„**ë¡œ ë”± ë§ëŠ” ê¸°ì‚¬
- ì˜ˆ: "í˜ˆì••ì´ ìê¾¸ íŠ„ë‹¤ë©´ ê¼­ í•´ì•¼ í•  ê²€ì‚¬", "ë¬´ë¦ ê´€ì ˆì´ ì•„í”Œ ë•Œ ì ˆëŒ€ í•˜ë©´ ì•ˆ ë˜ëŠ” ë™ì‘" ë“±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥

ë˜í•œ, ê¸°ì‚¬ ì£¼ì œë¥¼ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
- "high" / "medium" / "low" (youtube_search_demand)

============================================================
3. ì²« í›…Â·ì œëª© ì ì¬ë ¥ (hook_potential_score)
============================================================

ì´ ê¸°ì‚¬ê°€ **ì²« 3ì´ˆ í›… + ì œëª© ì•ë¶€ë¶„**ìœ¼ë¡œ ë½‘ê¸° ì¢‹ì€ì§€ í‰ê°€í•˜ì„¸ìš”.

hook_potential_score:

1ì :
- ìˆ˜ì¹˜, ìˆœìœ„, ê·¹ë‹¨ì  ëŒ€ë¹„, "ëª¨ë¥´ê³  í•˜ë©´ ì†í•´" ê°™ì€ í›… í¬ì¸íŠ¸ê°€ ê±°ì˜ ì—†ìŒ
- ì„¤ëª…/ë³´ê³ /í˜„í™© ìœ„ì£¼ ê¸°ì‚¬

2ì :
- í›…ìœ¼ë¡œ ì“¸ ë§Œí•œ ë¬¸ì¥ì€ ì–µì§€ë¡œ ë½‘ìœ¼ë©´ í•˜ë‚˜ ì •ë„ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë‚˜,
  ì„íŒ©íŠ¸ê°€ ì•½í•¨

3ì :
- "ì™œ â—‹â—‹í•œ ì‚¬ëŒì€ â—‹â—‹í•˜ê³ , ì–´ë–¤ ì‚¬ëŒì€ â—‹â—‹í• ê¹Œ" ìˆ˜ì¤€ì˜ í›…ì€ ë§Œë“¤ ìˆ˜ ìˆìŒ
- ê·¸ë˜ë„ ì•„ì£¼ ê°•ë ¥í•˜ì§„ ì•ŠìŒ

4ì :
- ìˆ«ì, ìœ„í—˜ ì¦ê°€, ì˜ì‚¬ë“¤ì´ ì‹«ì–´í•˜ëŠ” ìŠµê´€, ë§ì´ í•˜ëŠ” ì‹¤ìˆ˜ ë“±
  í›…ìœ¼ë¡œ ì“°ê¸° ì¢‹ì€ ë¬¸ì¥ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë– ì˜¤ë¥´ëŠ” ê¸°ì‚¬

5ì :
- "í˜ˆì••ì´ 3ë°° ë¹¨ë¦¬ ë§ê°€ì§€ëŠ” ìŠµê´€", "ë¬´ë¦ì´ ì¡°ìš©íˆ ë‹³ì•„ë²„ë¦¬ëŠ” ìˆœê°„"ì²˜ëŸ¼
  **ê°•ë ¥í•œ í•œ ì¤„ í›…**ì„ ì—¬ëŸ¬ ê°œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê¸°ì‚¬

============================================================
4. ë°˜ì „ + í•˜ëª¬ ì„œí´ ìŠ¤í† ë¦¬ ì ì¬ë ¥ (reversal_story_score)
============================================================

ì´ ê¸°ì‚¬ê°€ **ë°˜ì „ + í•˜ëª¬ ì„œí´ êµ¬ì¡°**ë¡œ í’€ê¸° ì¢‹ì€ì§€ í‰ê°€í•˜ì„¸ìš”.

- ê¸°ë³¸ êµ¬ì¡°:
  - í‰ë²”í•˜ê²Œ ë§ì´ í•˜ëŠ” í–‰ë™/ìŠµê´€
  - ê·¸ëŸ°ë° ê·¸ê²Œ ì˜¤íˆë ¤ ëª¸ì„ í˜ë“¤ê²Œ ë§Œë“ ë‹¤ëŠ” ë°˜ì „
  - ëª°ëì„ ë•Œ vs ì•Œì•˜ì„ ë•Œ ê²°ê³¼ ëŒ€ë¹„
  - ë§ˆì§€ë§‰ì— ìƒí™œ ìŠµê´€/ì²´í¬ë¦¬ìŠ¤íŠ¸ë¡œ ë§ˆë¬´ë¦¬

reversal_story_score:

1ì :
- ë‹¨ìˆœ ì •ë³´ ë‚˜ì—´, ì—°êµ¬ ê²°ê³¼ ìš”ì•½ë§Œ ìˆê³ ,
  "ë§ì´ë“¤ ì´ë ‡ê²Œ ì•Œê³  ìˆëŠ”ë° ì‚¬ì‹¤ì€ ë°˜ëŒ€" ê°™ì€ êµ¬ì¡°ê°€ ê±°ì˜ ì—†ìŒ

2ì :
- ë°˜ì „ ë¹„ìŠ·í•œ í¬ì¸íŠ¸ê°€ ì•„ì˜ˆ ì—†ì§„ ì•Šì§€ë§Œ,
  ì•„ì£¼ ì•½í•˜ê³ , ìŠ¤í† ë¦¬ë¡œ í’€ê¸° ì• ë§¤í•¨

3ì :
- "ë³´í†µ ì´ë ‡ê²Œë“¤ í•˜ëŠ”ë°, ì‚¬ì‹¤ì€ ì´ëŸ° ì ì„ ì¡°ì‹¬í•´ì•¼ í•œë‹¤" ì •ë„ì˜
  ê°€ë²¼ìš´ ë°˜ì „ì€ ê°€ëŠ¥

4ì :
- "ì¢‹ë‹¤ê³  ë¯¿ê³  í•œ í–‰ë™ì´ ì˜¤íˆë ¤ ë¬¸ì œ", "í”¼í•˜ë ¤ê³  í•œ í–‰ë™ì´ ì˜¤íˆë ¤ ë…"ì²˜ëŸ¼
  ìŠ¤í† ë¦¬í˜• ë°˜ì „ì´ ì˜ ë‚˜ì˜¤ëŠ” ê¸°ì‚¬

5ì :
- í•˜ëª¬ ì„œí´ ì „ì²´ë¥¼ ê¹”ë”í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆì„ ì •ë„ë¡œ
  'ê¸°ëŒ€ì™€ í˜„ì‹¤ì˜ ê´´ë¦¬', 'ì•Œë˜ ìƒì‹ì˜ ë’¤ì§‘ê¸°' í¬ì¸íŠ¸ê°€ ëšœë ·í•¨

============================================================
5. main_search_keyword ì„ ì •
============================================================

- ì´ ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ,
  ë‚˜ì¤‘ì— ìœ íŠœë¸Œ ëŒ€ë³¸ì—ì„œ **ì²« í›…ê³¼ ì œëª© ì•ë¶€ë¶„ì— ì“¸ í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œ**ë¥¼ í•˜ë‚˜ ë½‘ìœ¼ì„¸ìš”.
- ê°€ëŠ¥í•˜ë©´ ìœ„ì— ë‚˜ì—´ëœ ëŒ€í‘œ í‚¤ì›Œë“œ ì•ˆì—ì„œ ê³ ë¥´ë˜,
  ë‚´ìš©ê³¼ ë§ì§€ ì•Šìœ¼ë©´ ê°€ì¥ ì¼ë°˜ì ì¸ ìƒìœ„ ê°œë…ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

ì˜ˆ)
- íƒ„ì†Œë§¤íŠ¸ â†’ "ì „ê¸°ì¥íŒ", ë˜ëŠ” "ê²¨ìš¸ ë‚œë°©ìš©í’ˆ"
- íŠ¹ì • ë³‘ì› â—‹â—‹ ì´ë²¤íŠ¸ â†’ "ê±´ê°•ê²€ì§„", "í˜ˆì•• ê²€ì‚¬", "ë‹¹ë‡¨ ê´€ë¦¬" ë“±
- íŠ¹ì • ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ ë¸Œëœë“œ â†’ "ì˜ì–‘ì œ", "ë¹„íƒ€ë¯¼D", "ì˜¤ë©”ê°€3" ë“±

============================================================
6. ë³‘ì›Â·ê¸°ê´€Â·ê¸°ì—… í™ë³´ ì—¬ë¶€
============================================================

- ì´ ê¸°ì‚¬ê°€ ë³‘ì›/ê¸°ê´€/ê¸°ì—…/ì§€ìì²´/í•™íšŒ í™ë³´Â·ìˆ˜ìƒÂ·í–‰ì‚¬ ì¤‘ì‹¬ì´ë©´
  is_hospital_news = true
- ê·¸ ì™¸ëŠ” false

============================================================
7. ìµœì¢… score ê³„ì‚° ê·œì¹™
============================================================

1) usefulness_score, search_demand_score, hook_potential_score, reversal_story_scoreë¥¼ ê°ê° 1~5ë¡œ ë¨¼ì € ì •í•©ë‹ˆë‹¤.

2) ê¸°ë³¸ì ìœ¼ë¡œ ë„¤ ê°€ì§€ì˜ í‰ê· ì„ ë°”íƒ•ìœ¼ë¡œ scoreë¥¼ 1~5 ì‚¬ì´ ì •í•˜ë˜,
   ë‹¤ìŒ ì œì•½ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”.

- search_demand_scoreê°€ 1~2ì´ë©´:
  â†’ ìµœì¢… scoreëŠ” **ìµœëŒ€ 2ì **ê¹Œì§€ë§Œ ì¤ë‹ˆë‹¤.

- hook_potential_scoreê°€ 1~2ì´ê±°ë‚˜
  reversal_story_scoreê°€ 1~2ì´ë©´:
  â†’ ìµœì¢… scoreëŠ” **ìµœëŒ€ 3ì **ê¹Œì§€ë§Œ ì¤ë‹ˆë‹¤.

- usefulness_scoreê°€ 1~2ì´ë©´:
  â†’ ì•„ë¬´ë¦¬ ê²€ìƒ‰ ìˆ˜ìš”ê°€ ë†’ì•„ë„ scoreëŠ” 3ì ê¹Œì§€ë§Œ í—ˆìš©í•©ë‹ˆë‹¤.

3) ë”°ë¼ì„œ,
   - ì•„ë¬´ë¦¬ ì¢‹ì€ ë‚´ìš©ì´ì–´ë„, ê²€ìƒ‰ ìˆ˜ìš”ê°€ ë‚®ìœ¼ë©´ ìµœì¢… scoreëŠ” 1~2ì ì— ë¨¸ë¬¼ëŸ¬ì•¼ í•©ë‹ˆë‹¤.
   - ê²€ìƒ‰ ìˆ˜ìš”ê°€ ë†’ê³ , í›…/ë°˜ì „ ì ì¬ë ¥ì´ ì¢‹ê³ , ë‚´ìš©ë„ ì‹¤ìš©ì ì¼ ë•Œë§Œ 4~5ì ì„ ì¤ë‹ˆë‹¤.

============================================================
8. ì¶œë ¥ í˜•ì‹ (JSON ONLY)
============================================================

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

{{
  "usefulness_score": 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜,
  "search_demand_score": 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜,
  "hook_potential_score": 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜,
  "reversal_story_score": 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜,
  "score": 1ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜,  # ìœ„ ê·œì¹™ì— ë”°ë¥¸ ìµœì¢… ì ìˆ˜
  "is_hospital_news": true ë˜ëŠ” false,
  "youtube_search_demand": "high" ë˜ëŠ” "medium" ë˜ëŠ” "low",
  "main_search_keyword": "ìœ íŠœë¸Œì—ì„œ ì•ì— ë°•ì„ í•µì‹¬ í‚¤ì›Œë“œ í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ì ˆ",
  "short_reason": "ì´ ê¸°ì‚¬ê°€ ëª‡ ì ì¸ì§€, í•˜ëª¬ ì„œí´/í›…/ë°˜ì „ ê´€ì ê¹Œì§€ í¬í•¨í•´ì„œ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ì„¤ëª…"
}}
"""
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        data = json.loads(resp.choices[0].message.content)

        # ê¸°ì¡´ ì½”ë“œì™€ ìµœëŒ€í•œ í˜¸í™˜ë˜ê²Œ ìœ ì§€
        score = int(data.get("score", 1))
        is_hospital_news = bool(data.get("is_hospital_news", False))
        reason = data.get("short_reason", "")

        # í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— main_search_keyword, youtube_search_demand,
        # hook_potential_score, reversal_story_score ë“±ë„ ì¨ë¨¹ì„ ìˆ˜ ìˆìŒ
        # main_kw = data.get("main_search_keyword", "")
        # demand_level = data.get("youtube_search_demand", "medium")

        return score, is_hospital_news, reason

    except Exception as e:
        print("[ê¸°ì‚¬í‰ê°€] ì˜¤ë¥˜:", e)
        return 1, True, "í‰ê°€ ì‹¤íŒ¨"


def pick_best_article_for_seniors(client: OpenAI, articles: list):
    """
    scrape_health_chosun_with_selenium()ì—ì„œ ê°€ì ¸ì˜¨ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¤‘
    'ì¡°íšŒìˆ˜ ì ì¬ë ¥ + ì‹œë‹ˆì–´ ë„ì›€' ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ ê¸°ì‚¬ 1ê°œë¥¼ ì„ íƒ.

    - search_demand_score > usefulness_score ìš°ì„ 
    - main_search_keywordê°€ í•µì‹¬ ê±´ê°• í‚¤ì›Œë“œì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ì¶”ê°€
    """

    BEST = None
    BEST_SCORE = -1

    for a in articles:
        title = a["title"]
        content = a["content"]

        score, is_hospital_news, reason = score_article_for_seniors(client, title, content)

        print(f"[ê¸°ì‚¬ í‰ê°€] '{title[:25]}...' -> {score}ì , ë³‘ì›í™ë³´:{is_hospital_news}, ì´ìœ :{reason}")

        # ================
        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        # ================
        # 1) ì ìˆ˜ ê·¸ëŒ€ë¡œ ê¸°ë³¸ ì ìˆ˜
        weighted_score = score

        # 2) ê²€ìƒ‰ ìˆ˜ìš”ê°€ ë†’ìœ¼ë©´ ì¶”ê°€ ê°€ì¤‘ì¹˜
        #    (ëŒ€ë³¸ ì¡°íšŒìˆ˜ ìƒìœ„ ìš”ì¸)
        if "í˜ˆì••" in title or "í˜ˆë‹¹" in title or "ì¹˜ë§¤" in title or "ì‹¬ì¥" in title or "ë‡Œ" in title:
            weighted_score += 2
        elif "ìˆ˜ë©´" in title or "ê´€ì ˆ" in title or "ë‹¤ì´ì–´íŠ¸" in title:
            weighted_score += 1

        # 3) ë³‘ì›í™ë³´ë¼ë„ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ê°•í•˜ë©´ ê°ì í•˜ì§€ ì•ŠìŒ
        if is_hospital_news:
            weighted_score -= 1  # ì‘ì€ í˜ë„í‹°ë§Œ ì¤˜ì„œ íƒˆë½ì‹œí‚¤ì§€ ì•ŠìŒ

        # ì¶œë ¥
        print(f" â†’ ê°€ì¤‘ì¹˜ ì ìš© í›„ ìµœì¢…ì ìˆ˜: {weighted_score}")

        # ìµœì¢… ì„ íƒ
        if weighted_score > BEST_SCORE:
            BEST_SCORE = weighted_score
            BEST = a

    if not BEST:
        print("[ê¸°ì‚¬ ì„ íƒ] ì˜¤ëŠ˜ ì‚¬ìš©í•  ë§Œí•œ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"[ê¸°ì‚¬ ì„ íƒ] ìµœì¢… ì„ íƒ: '{BEST['title']}' (ìµœì¢… ì ìˆ˜ {BEST_SCORE})")
    return BEST

# ==============================================================================
# --- ğŸ–‹ï¸ 2ë‹¨ê³„: GPT ëŒ€ë³¸ ë° í‚¤ì›Œë“œ ìƒì„± ëª¨ë“ˆ (íšŒì¶˜41 ì „ìš© ì‹ ê·œ ë²„ì „) ---
# ==============================================================================
def generate_script_for_seniors(client: OpenAI, article_title: str, article_content: str):
    print("\n" + "="*50 + "\nğŸ–‹ï¸ 2ë‹¨ê³„: í†µí•© ëŒ€ë³¸ ë° ìƒí’ˆ ì „ëµ ìƒì„± (ê°€ì„±ë¹„ ëª¨ë“œ)...\n" + "="*50)

    prompt = f"""
ë‹¹ì‹ ì€ 4070 ì‹œì²­ìê°€ ì§€ë£¨í•  í‹ˆ ì—†ì´ ë¹ ì ¸ë“¤ê²Œ ë§Œë“œëŠ” **'ìœ„íŠ¸ ìˆëŠ” ê±´ê°• ì „ë¬¸ê°€'**ì…ë‹ˆë‹¤.
ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë³¸ì„ ì“°ë˜, **í˜¸í¡ì„ ì•„ì£¼ ì§§ê³  ê²½ì¾Œí•˜ê²Œ** ëŠì–´ì¹˜ì„¸ìš”. ë¬¸ì¥ì´ ê¸¸ì–´ì§€ë©´ ì‹œì²­ìëŠ” ë°”ë¡œ ë‚˜ê°‘ë‹ˆë‹¤.

============================================================
ğŸš« [í•µì‹¬ ê·œì¹™]
============================================================
1. **ì‰¬ìš´ ë‹¨ì–´ ì‚¬ìš©**: "í•­ì‚°í™” ì‘ìš©", "ë©”ì»¤ë‹ˆì¦˜" ê°™ì€ ë§ ëŒ€ì‹  **"ë…¹ìŠ¨ ëª¸ì„ ë‹¦ì•„ì¤€ë‹¤", "ì‘ë™ ì›ë¦¬"** ì²˜ëŸ¼ ì‰¬ìš´ ë¹„ìœ ë¥¼ ë“œì„¸ìš”.
2. **ë°°ìš°ëŠ” ì¦ê±°ì›€**: ë‹¨ìˆœíˆ ê²ì£¼ëŠ” ê²Œ ì•„ë‹ˆë¼, **"ì•„, ê·¸ë˜ì„œ ê·¸ëŸ° ê±°êµ¬ë‚˜!"** í•˜ê³  ë¬´ë¦ì„ íƒ ì¹˜ê²Œ ë§Œë“œëŠ” ì§€ì‹/ìƒì‹ì„ ì „ë‹¬í•˜ì„¸ìš”.
3. **ë¸”ë¼ì¸ë“œ ìœ ì§€**: ëŒ€ë³¸ì—ì„œëŠ” í•µì‹¬ ì„±ë¶„/ì œí’ˆëª…ì„ **"ì´ê²ƒ", "ì´ ì„±ë¶„"**ìœ¼ë¡œ ëê¹Œì§€ ìˆ¨ê¸°ì„¸ìš”.
4. **ê²€ìƒ‰ì–´ëŠ” ì •ë‹µ ê³µê°œ (ì¤‘ìš”)**: ì˜ìƒì—ì„  ìˆ¨ê¸°ì§€ë§Œ, ì•„ë˜ JSON ë°ì´í„°ì˜ `product_search_keywords`ì—ëŠ” **ì‹¤ì œ ì„±ë¶„ëª…/ì œí’ˆëª…(ì˜ˆ: ë¦¬í¬ì¢€ ë¹„íƒ€ë¯¼C, ë³´ìŠ¤ì›°ë¦¬ì•„)**ì„ ì •í™•íˆ ì ì–´ì•¼ í•©ë‹ˆë‹¤. ì¶”ìƒì ì¸ ë‹¨ì–´ ê¸ˆì§€!
5. **CTA ëª…í™•íˆ**: "ëŒ“ê¸€"ì´ ì•„ë‹™ë‹ˆë‹¤. **"ì±„ë„ëª…(íšŒì¶˜41) ëˆ„ë¥´ê³  ë§í¬ í™•ì¸"**ìœ¼ë¡œ ìœ ë„í•˜ì„¸ìš”.

============================================================
ğŸ”¥ [ëŒ€ë³¸ êµ¬ì¡° - í‹°í‚¤íƒ€ì¹´ ë¯¸ìŠ¤í„°ë¦¬]
============================================================
**ì´ ë¶„ëŸ‰: 7~9ë¬¸ì¥ (ì§§ê²Œ ìª¼ê°œì„œ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¼)**

1. **[ê³µê° ì§ˆë¬¸ (1ì´ˆ ì»·)]**:
   - ì§§ê³  êµµê²Œ ë¬¼ì–´ë³´ì„¸ìš”.
   - ì˜ˆ: "ê±°ìš¸ ë³¼ ë•Œë§ˆë‹¤ í•œìˆ¨ ë‚˜ì˜¤ì‹œë‚˜ìš”?"

2. **[ì¦‰ë¬¸ì¦‰ë‹µ (ì†”ë£¨ì…˜)]**:
   - ë‹µì„ ë°”ë¡œ ë˜ì§€ì„¸ìš”. (ì´ë¦„ì€ ìˆ¨ê¸°ê³ )
   - ì˜ˆ: "ê·¸ëŸ´ ë• **'ì´ê²ƒ'**ì´ ì •ë‹µì…ë‹ˆë‹¤."

3. **[íš¨ëŠ¥ í­ê²© (íƒ! íƒ! íƒ!)]**:
   - ì¥ì ì„ ì§§ê²Œ ë‚˜ì—´í•˜ì„¸ìš”.
   - ì˜ˆ: "í”¼ë¶€ë§Œ ì¢‹ì•„ì§ˆê¹Œìš”? ì•„ë‹™ë‹ˆë‹¤."
   - ì˜ˆ: "í”¼ë¡œëŠ” ì‹¹ í’€ë¦¬ê³ , ë©´ì—­ë ¥ì€ ì‘¥ ì˜¬ë¼ê°‘ë‹ˆë‹¤."

4. **[ìœ„ê¸° ì¡°ì„± (ë°˜ì „)]**:
   - ì§§ê²Œ ê²½ê³ í•˜ì„¸ìš”.
   - ì˜ˆ: "í•˜ì§€ë§Œ ì•„ë¬´ê±°ë‚˜ ë¨¹ìœ¼ë©´? ëˆë§Œ ë²„ë¦¬ëŠ” ê²ë‹ˆë‹¤."
   - ì˜ˆ: "í¡ìˆ˜ìœ¨ì´ ê½ì´ê±°ë“ ìš”."

5. **[ê¸°ì¤€ ì œì‹œ (í•µì‹¬)]**:
   - ë”± í•˜ë‚˜ë§Œ ê¸°ì–µí•˜ë¼ê³  í•˜ì„¸ìš”.
   - ì˜ˆ: "ë”± í•˜ë‚˜, **'ë¦¬í¬ì¢€ ê³µë²•'**ì¸ì§€ë§Œ í™•ì¸í•˜ì„¸ìš”."

6. **[ê²°ì •íƒ€ (CTA)]**:
   - ì˜ˆ: "ë„ëŒ€ì²´ ì´ê²Œ ë­˜ê¹Œìš”?"
   - ì˜ˆ: "ì•„ë˜ **ì œ ì±„ë„ëª… 'íšŒì¶˜41'ì„ ëˆ„ë¥´ê³ ** ë§í¬ì—ì„œ ì •ë‹µì„ í™•ì¸í•˜ì„¸ìš”."
   - ê·¸ í›„ "{EFFECT_TRIGGER_TEXT}"

============================================================
ğŸ“¦ [ì‘ë‹µ í˜•ì‹ â€” JSON Only]
============================================================
{{
  "youtube_title": "ì˜ìƒ ìƒë‹¨ ì œëª© (15ì ì´ë‚´, í•´ì‹œíƒœê·¸ ê¸ˆì§€, ë¬´ì¡°ê±´ ì§§ê³  ê°•ë ¬í•˜ê²Œ)",
  "thumbnail_text": "ì¸ë„¤ì¼ ë¬¸êµ¬ (ì˜ˆ: 99%ê°€ ëª¨ë¥´ëŠ” ë¹„ë²•)",
  "thumbnail_segments": [
    {{"text": "í•µì‹¬", "color": "red"}},
    {{"text": "ë‚˜ë¨¸ì§€", "color": "white"}}
  ],
  "script": "ë¬¸ì¥1(ì§§ì€ì§ˆë¬¸)\\në¬¸ì¥2(ì§§ì€ë‹µ)\\në¬¸ì¥3(íš¨ëŠ¥1)\\në¬¸ì¥4(íš¨ëŠ¥2)\\në¬¸ì¥5(ê²½ê³ )\\në¬¸ì¥6(ê¸°ì¤€)\\në¬¸ì¥7(ê¶ê¸ˆí•˜ì£ ?)\\në¬¸ì¥8(ì±„ë„ëª…ëˆ„ë¥´ê³ í™•ì¸)\\n{EFFECT_TRIGGER_TEXT}",
  "search_keywords": ["visual_keyword", "í•µì‹¬í‚¤ì›Œë“œ1"],
  "product_strategy": {{
    "should_recommend_product": true,
    "product_search_keywords": ["ì‹¤ì œ_ì„±ë¶„ëª…_ë˜ëŠ”_ì •í™•í•œ_ì œí’ˆëª…_í‚¤ì›Œë“œ"],
    "product_mention_label": "ì˜ìƒ ë©˜íŠ¸ìš© ë¼ë²¨(ì˜ˆ: ì •ë‹µ í™•ì¸í•˜ê¸°)",
    "product_reason": "ì˜ìƒì—ì„œëŠ” ìˆ¨ê²¼ì§€ë§Œ, ì´ ì œí’ˆì´ ëŒ€ë³¸ì˜ ê¸°ì¤€(ì˜ˆ: ë¦¬í¬ì¢€, ì €ë¶„ì)ì„ ì¶©ì¡±í•˜ëŠ” ì‹¤ì œ ì œí’ˆì„"
  }}
}}

[ê¸°ì‚¬ ì œëª©]
{article_title}

[ê¸°ì‚¬ ë‚´ìš©]
{article_content[:2500]}
"""

    try:
        response = client.chat.completions.create(
            model="gpt-5.1",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            response_format={"type": "json_object"}
        )

        script_data = json.loads(response.choices[0].message.content)
        
        # [ê°•ì œ QC] ì œëª© í•´ì‹œíƒœê·¸ ë° ë§ˆì§€ë§‰ ë¬¸ì¥ ë³´ì • (ë¹„ìš© 0ì›)
        if "#ê±´ê°•" not in script_data.get("youtube_title", ""):
             script_data["youtube_title"] = re.sub(r'#\S+', '', script_data.get("youtube_title", "")).strip() + " #ê±´ê°•ì •ë³´ #ê±´ê°•"
        
        lines = [l.strip() for l in script_data.get("script", "").split("\n") if l.strip()]
        if not lines or lines[-1] != EFFECT_TRIGGER_TEXT:
            lines.append(EFFECT_TRIGGER_TEXT)
        script_data["script"] = "\n".join(lines)
        
        # ìƒí’ˆ ì „ëµ ê°•ì œ í™œì„±í™”
        if "product_strategy" not in script_data:
            script_data["product_strategy"] = {"should_recommend_product": True, "product_search_keywords": []}
        script_data["product_strategy"]["should_recommend_product"] = True

        print("  âœ… [ì„±ê³µ] í†µí•© ëŒ€ë³¸ ë° ìƒí’ˆ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ.")
        return script_data

    except Exception as e:
        print(f"  - [ì˜¤ë¥˜] GPT í†µí•© ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    
# ==============================================================================
# --- ğŸ›’ ì¿ íŒ¡ ìƒí’ˆ ê²€ìƒ‰ + ë§í¬íŠ¸ë¦¬ ì—…ë¡œë“œ ëª¨ë“ˆ ---
# ==============================================================================
def load_coupang_config():
    try:
        with open(COUPANG_CONFIG_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[ì¿ íŒ¡] ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def _coupang_sign(method: str, uri: str, secret_key: str, access_key: str):
    """
    ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ HMAC ì„œëª… ìƒì„± (ê³µì‹ í¬ë§· ë§ì¶¤)
    """
    path, _, query = uri.partition('?')
    date_gmt = time.strftime('%y%m%d', time.gmtime())
    time_gmt = time.strftime('%H%M%S', time.gmtime())
    datetime_str = date_gmt + 'T' + time_gmt + 'Z'
    message = datetime_str + method + path + (query or "")
    signature = hmac.new(secret_key.encode('utf-8'), msg=message.encode('utf-8'), digestmod=hashlib.sha256).hexdigest()
    return f"CEA algorithm=HmacSHA256, access-key={access_key}, signed-date={datetime_str}, signature={signature}"

def search_coupang_products(keyword: str, max_results: int = 3):
    """
    [ì˜¤ë¥˜ ìˆ˜ì •íŒ] 
    - ì¸ì½”ë”© ë° ì‘ë‹µ íŒŒì‹± ë¡œì§ ê°•í™”
    """
    config = load_coupang_config()
    if not config: return []

    access_key = config["access_key"]
    secret_key = config["secret_key"]
    method = "GET"
    
    # ğŸš¨ ì¸ì½”ë”© ì£¼ì˜: quoteë¥¼ í•œ ë²ˆë§Œ ì ìš©
    q = requests.utils.quote(keyword)
    # limit ìµœëŒ€ì¹˜ëŠ” 10ì…ë‹ˆë‹¤ (50ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì—ëŸ¬ ë°œìƒ)
    uri = f"/v2/providers/affiliate_open_api/apis/openapi/products/search?keyword={q}&limit=10"

    auth = _coupang_sign(method, uri, secret_key, access_key)
    url = "https://api-gateway.coupang.com" + uri
    headers = {"Authorization": auth, "Content-Type": "application/json;charset=UTF-8"}

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code != 200:
            print(f"[ì¿ íŒ¡] API ì˜¤ë¥˜ ({resp.status_code}): {resp.text}")
            return []

        data = resp.json()
        items = []
        
        # ì‘ë‹µ êµ¬ì¡° ìœ ì—°í•˜ê²Œ íƒìƒ‰
        if "data" in data:
            d = data["data"]
            if isinstance(d, list): items = d
            elif isinstance(d, dict):
                items = d.get("productData", []) or d.get("items", [])

        if not items:
            print(f"[ì¿ íŒ¡] '{keyword}' ê²€ìƒ‰ ê²°ê³¼ê°€ 0ê±´ì…ë‹ˆë‹¤.")
            print(f"  ğŸ” [ë””ë²„ê·¸] API ì‘ë‹µ ì „ì²´: {json.dumps(data, ensure_ascii=False)[:500]}...") # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶”ê°€
            return []

        filtered = []
        for item in items:
            p_url = item.get("productUrl") or item.get("link")
            p_name = item.get("productName") or item.get("title")
            if not p_url or not p_name: continue

            r_count = item.get("ratingCount") or 0
            filtered.append({
                "title": p_name,
                "url": p_url,
                "image": item.get("productImage") or item.get("imageUrl"),
                "price": item.get("productPrice") or item.get("price"),
                "rating_count": int(r_count)
            })

        filtered.sort(key=lambda x: x["rating_count"], reverse=True)
        print(f"[ì¿ íŒ¡] '{keyword}' ê²€ìƒ‰ ì„±ê³µ! ({len(filtered)}ê°œ ë°œê²¬)")
        return filtered[:max_results]

    except Exception as e:
        print(f"[ì¿ íŒ¡] ì˜ˆì™¸ ë°œìƒ: {e}")
        return []

def verify_product_relevance(client: OpenAI, script_title: str, product_name: str) -> bool:
    """
    GPTë¥¼ ì´ìš©í•´ ì´ ìƒí’ˆì´ ëŒ€ë³¸ ë‚´ìš©ê³¼ ì •ë§ ê´€ë ¨ì´ ìˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    ì˜ˆ: ëŒ€ë³¸(ë¬´ë¦ í†µì¦) - ìƒí’ˆ(íƒˆëª¨ ìƒ´í‘¸) -> False
    """
    prompt = f"""
    Context: The YouTube video title is "{script_title}".
    Product: "{product_name}"
    
    Task: determine if this product is logically relevant to the video topic.
    - If the video is about 'Knee Pain' and product is 'Knee Brace' or 'MSM', return TRUE.
    - If the video is about 'Sleep' and product is 'Coffee' or 'Shampoo', return FALSE.
    - Be generous if it's somewhat related.
    
    Output only "TRUE" or "FALSE".
    """
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        ans = resp.choices[0].message.content.strip().upper()
        return "TRUE" in ans
    except Exception:
        return True # ì—ëŸ¬ë‚˜ë©´ ê·¸ëƒ¥ í†µê³¼

def is_product_relevant(product_title: str, keyword: str) -> bool:
    """
    ìƒí’ˆëª…ì´ í‚¤ì›Œë“œì™€ ì–´ëŠ ì •ë„ ê´€ë ¨ ìˆëŠ”ì§€ ì²´í¬.
    - í‚¤ì›Œë“œë¥¼ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆˆ ë’¤
    - ë‘ ê¸€ì ì´ìƒ í† í°ì´ ì œëª©ì— 1ê°œ ì´ìƒ í¬í•¨ë˜ë©´ 'ê´€ë ¨ ìˆìŒ'ìœ¼ë¡œ ë´„
    """
    if not product_title or not keyword:
        return False

    title = product_title.replace(" ", "").lower()
    tokens = [t.strip() for t in re.split(r'[\s,/]+', keyword) if t.strip()]

    hit = 0
    for t in tokens:
        t_norm = t.replace(" ", "").lower()
        if len(t_norm) <= 1:
            continue
        if t_norm in title:
            hit += 1

    return hit >= 1

def post_product_to_linktree(product: dict, link_title: str, category: str = "ì‹œë‹ˆì–´ ê±´ê°•"):
    """
    link_title: ë§í¬íŠ¸ë¦¬ì— í‘œì‹œí•  ì œëª© (ìš°ë¦¬ê°€ ì˜ˆì˜ê²Œ ê°€ê³µí•œ ì œëª©)
    """
    if not product:
        print("[ë§í¬íŠ¸ë¦¬] ì „ì†¡í•  ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    payload = {
        "title": link_title,              # ğŸ”¹ ì—¬ê¸°!
        "url": product.get("url"),
        "image": product.get("image"),
        # categoryëŠ” ì•„ì§ DB ì»¬ëŸ¼ì´ ì—†ìœ¼ë‹ˆê¹Œ ì„œë²„ì—ì„œëŠ” ë¬´ì‹œí•´ë„ ë¨
    }

    headers = {
        "x-api-key": API_SECRET,
    }

    try:
        resp = requests.post(LINKTREE_ADMIN_API, json=payload, headers=headers, timeout=10)
        if resp.status_code == 200:
            print("[ë§í¬íŠ¸ë¦¬] ë§í¬ ë“±ë¡ ì™„ë£Œ:", resp.json())
        else:
            print(f"[ë§í¬íŠ¸ë¦¬] ë“±ë¡ ì‹¤íŒ¨ status={resp.status_code}, body={resp.text}")
    except Exception as e:
        print(f"[ë§í¬íŠ¸ë¦¬] í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

def add_related_coupang_product(client: OpenAI, script_data: dict):
    """
    [ì§€ëŠ¥í˜• ìˆ˜ì •ë³¸]
    1. ë¦¬ë·° ë§ì€ ìˆœìœ¼ë¡œ ìƒí’ˆì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    2. GPTì—ê²Œ 'ì´ ìƒí’ˆì´ ì§„ì§œ ëŒ€ë³¸ê³¼ ë§ëŠ”ì§€' ë¬¼ì–´ë³´ê³  í†µê³¼í•œ ê²ƒë§Œ ì¶”ì²œí•©ë‹ˆë‹¤.
    3. ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ë‹¨ìˆœí™” í›„ ì¬ì‹œë„.
    """
    strategy = script_data.get("product_strategy") or {}
    should = strategy.get("should_recommend_product", False)
    script_title = script_data.get("youtube_title", "")

    if not should:
        print("[ì¿ íŒ¡] ìƒí’ˆ ì¶”ì²œì„ í•˜ì§€ ì•Šê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.")
        return None

    keywords = strategy.get("product_search_keywords") or []
    if not keywords:
        print("[ì¿ íŒ¡] í‚¤ì›Œë“œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return None

    selected_product = None
    selected_keyword = None
    backup_product = None
    backup_keyword = None

    for kw in keywords:
        kw_clean = kw.strip()
        print(f"[ì¿ íŒ¡] 1ì°¨ ì‹œë„: '{kw_clean}' ê²€ìƒ‰ (ë¦¬ë·°ìˆœ)...")
        
        # 1ì°¨ ê²€ìƒ‰ (ìµœëŒ€ 10ê°œ í›„ë³´)
        products = search_coupang_products(kw_clean, max_results=10)
        
        # ğŸš¨ ì‹¤íŒ¨ ì‹œ ìë™ ë‹¨ìˆœí™” ì¬ê²€ìƒ‰
        if not products and " " in kw_clean:
            # ë‹¨ìˆœ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ì•„ë‹ˆë¼, 'ê°€ì¥ ê¸´ ë‹¨ì–´'ë¥¼ í•µì‹¬ í‚¤ì›Œë“œë¡œ ê°„ì£¼ (ì˜ˆ: "ì‹¤ë‚´ íŠ¸ë¨í„ë¦° 10ë¶„" -> "íŠ¸ë¨í„ë¦°")
            tokens = kw_clean.split()
            simple_kw = max(tokens, key=len) if tokens else kw_clean.split()[-1]

            if len(simple_kw) > 1:
                print(f"[ì¿ íŒ¡] âš ï¸ 1ì°¨ ì‹¤íŒ¨ -> í•µì‹¬ ë‹¨ì–´ '{simple_kw}'(ìœ¼)ë¡œ ì¬ê²€ìƒ‰ ì‹œë„ (ê¸°ì¡´: {kw_clean})...")
                products = search_coupang_products(simple_kw, max_results=10)
                if products:
                    kw_clean = simple_kw 

        if not products:
            continue

        # ğŸ”¹ [ì¶”ê°€] ê²€ì¦ í†µê³¼í•œ ê²Œ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„í•´, ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ì˜ 1ë“± ìƒí’ˆì„ ë°±ì—…í•´ë‘ 
        if not backup_product:
            backup_product = products[0]
            backup_keyword = kw_clean

        # âœ¨ [ìˆ˜ì •] GPT ê²€ì¦ ì‚­ì œ (ë„ˆë¬´ ê¹ê¹í•´ì„œ ê²€ìƒ‰ëœ ì¢‹ì€ ìƒí’ˆì„ ë‹¤ ë‚ ë¦¼)
        # ì¿ íŒ¡ ê²€ìƒ‰ ê²°ê³¼ëŠ” ì´ë¯¸ í‚¤ì›Œë“œ ì—°ê´€ì„±ì´ ë†’ìœ¼ë¯€ë¡œ, ê¸°ë³¸ í•„í„°ë§ë§Œ ê±°ì¹˜ê³  ë°”ë¡œ ì±„íƒ
        for p in products:
            # 1. ë¦¬ë·°ê°€ ë„ˆë¬´ ì ìœ¼ë©´(10ê°œ ë¯¸ë§Œ) ìŠ¤í‚µ (ì‹ ë¢°ë„ ë¬¸ì œ)
            if p["rating_count"] < 10:
                continue

            # 2. ì œëª© ìœ ì‚¬ì„± ì²´í¬ (ê¸°ë³¸) - í‚¤ì›Œë“œê°€ ìƒí’ˆëª…ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
            if is_product_relevant(p["title"], kw_clean):
                print(f"    âœ… ìƒí’ˆ ì„ íƒ ì™„ë£Œ: {p['title'][:30]}...")
                selected_product = p
                selected_keyword = kw_clean
                break
        
        if selected_product:
            break

    # ğŸ”¹ [ì¶”ê°€] ê²€ì¦ëœ ìƒí’ˆì€ ì—†ì§€ë§Œ, ê²€ìƒ‰ ê²°ê³¼ëŠ” ìˆì—ˆë˜ ê²½ìš° -> ë°±ì—… ìƒí’ˆ ì‚¬ìš©
    if not selected_product and backup_product:
        print(f"[ì¿ íŒ¡] ê²€ì¦ëœ ìƒí’ˆì€ ì—†ì§€ë§Œ, ê²€ìƒ‰ ê²°ê³¼({backup_keyword})ê°€ ìˆì–´ 1ìˆœìœ„ ìƒí’ˆì„ ì„ íƒí•©ë‹ˆë‹¤.")
        selected_product = backup_product
        selected_keyword = backup_keyword

    # ë§Œì•½ ê²€ì¦ í†µê³¼í•œ ê²Œ í•˜ë‚˜ë„ ì—†ìœ¼ë©´, ì œëª© ê¸°ë°˜ìœ¼ë¡œ ì¬ê²€ìƒ‰ ì‹œë„ (Fallback)
    if not selected_product:
        print("[ì¿ íŒ¡] âš ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨. ì œëª© ê¸°ë°˜ ì¬ê²€ìƒ‰ ì‹œë„...")
        # ì œëª©ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (ê°„ë‹¨í•˜ê²Œ: ì¡°ì‚¬ê°€ ì—†ëŠ” ê°€ì¥ ê¸´ ë‹¨ì–´ 2ê°œ)
        tokens = script_title.split()
        # í•´ì‹œíƒœê·¸ ì œê±°
        tokens = [t for t in tokens if not t.startswith("#")]
        # ê¸¸ì´ ìˆœ ì •ë ¬
        tokens.sort(key=len, reverse=True)
        
        fallback_keywords = tokens[:2] # ê°€ì¥ ê¸´ ë‹¨ì–´ 2ê°œ í›„ë³´
        
        for fb_kw in fallback_keywords:
            if len(fb_kw) < 2: continue # ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨ìŠ¤
            print(f"  -> ì¬ê²€ìƒ‰ ì‹œë„: '{fb_kw}'")
            products = search_coupang_products(fb_kw, max_results=5)
            
            if products:
                # ì²« ë²ˆì§¸ ì œí’ˆ ë°”ë¡œ ì„ íƒ (ê²€ì¦ ìƒëµ - ê¸‰í•˜ë‹ˆê¹Œ)
                selected_product = products[0]
                selected_keyword = fb_kw
                print(f"    âœ… ì œëª© ê¸°ë°˜ ê²€ìƒ‰ ì„±ê³µ! ({fb_kw})")
                break

    # ê·¸ë˜ë„ ì—†ìœ¼ë©´, ê·¸ëƒ¥ ë¦¬ë·° 1ë“± ìƒí’ˆ ì„ íƒ (ì•ˆ í•˜ëŠ” ê²ƒë³´ë‹¨ ë‚˜ìŒ - ê¸°ì¡´ ë¡œì§)
    # (ìœ„ ë£¨í”„ì—ì„œ productsê°€ ë‚¨ì•„ìˆë‹¤ë©´ ì‚¬ìš©)
    if not selected_product and products:
        print("[ì¿ íŒ¡] ê²€ì¦ í†µê³¼ ìƒí’ˆ ì—†ìŒ. ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ì˜ 1ìœ„ ìƒí’ˆ ê°•ì œ ì„ íƒ.")
        selected_product = products[0]
        selected_keyword = kw_clean

    if not selected_product:
        print("[ì¿ íŒ¡] ëª¨ë“  ì‹œë„ì—ë„ ì ì ˆí•œ ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None

    # ë§í¬íŠ¸ë¦¬ ë“±ë¡ ë¡œì§ (ìˆ˜ì •ë¨: ê¹”ë”í•˜ê²Œ ì •ë¦¬)
    # 1. ìœ íŠœë¸Œ ì œëª© ì •ë¦¬
    youtube_title = (script_data.get("youtube_title") or "").strip()
    clean_yt = re.sub(r'#\S+', '', youtube_title).strip()
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì„ íƒì‚¬í•­, ê¹”ë”í•˜ê²Œ ìœ ì§€)
    clean_yt = re.sub(r'[!?:;]', '', clean_yt).strip()
    
    # [ìˆ˜ì •] ìœ íŠœë¸Œ ì œëª© ê¸¸ì´ ì œí•œ í•´ì œ (ì „ì²´ ì œëª© ì‚¬ìš©)
    # if len(clean_yt) > 15: clean_yt = clean_yt[:15] + "..."

    # 2. ìƒí’ˆëª… ëŒ€ì‹  'ê²€ìƒ‰ í‚¤ì›Œë“œ' ì‚¬ìš© (ì˜ˆ: "ì¿¨ë§¤íŠ¸", "ì§€ì•• ìŠ¬ë¦¬í¼")
    clean_prod = selected_keyword

    link_title = f"[{clean_yt}] {clean_prod}"

    print(f"[ì¿ íŒ¡] ìµœì¢… ì„ íƒ: '{selected_keyword}' -> {clean_prod} (ë§í¬íŠ¸ë¦¬ ì œëª©: {link_title})")
    
    post_product_to_linktree(selected_product, link_title, category="ì‹œë‹ˆì–´ ê±´ê°•")

    return {
        "product": selected_product,
        "keyword": selected_keyword,
        "link_title": link_title,
        "clean_name": clean_prod
    }
def apply_final_closing_line(script_data: dict, selected_product: dict | None):
    """
    [ìˆ˜ì •ë¨] ë¯¸ìŠ¤í„°ë¦¬ ì „ëµ ìœ ì§€ë¥¼ ìœ„í•´ ìƒí’ˆëª… ì–¸ê¸‰ ë©˜íŠ¸ë¥¼ ê°•ì œë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ì˜¤ì§ í™©ê¸ˆì—´ì‡  ë©˜íŠ¸(EFFECT_TRIGGER_TEXT)ë§Œ ë§ˆì§€ë§‰ì— ë³´ì¥í•©ë‹ˆë‹¤.
    """
    script_text = (script_data.get("script") or "").strip()
    if not script_text:
        return script_data

    # 1. ê¸°ì¡´ ë©˜íŠ¸ í´ë¦¬ë‹ (ì¤‘ë³µ ë°©ì§€)
    clean_text = script_text.replace(EFFECT_TRIGGER_TEXT, "")
    
    lines = [l.strip() for l in clean_text.split("\n") if l.strip()]
    
    # 2. íŒŒì´ì¬ì—ì„œì˜ ê°•ì œ ë©˜íŠ¸ ì¶”ê°€ ì œê±° (GPTê°€ ì“´ ëŒ€ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    # ë¯¸ìŠ¤í„°ë¦¬ ì „ëµì—ì„œëŠ” ì—¬ê¸°ì„œ ìƒí’ˆëª…ì„ ë§í•´ë²„ë¦¬ë©´ ì•ˆë¨.
    
    # 3. ë§ˆì§€ë§‰ì— í™©ê¸ˆì—´ì‡  ë©˜íŠ¸ ë¬´ì¡°ê±´ 1ê°œ ì¶”ê°€
    lines.append(EFFECT_TRIGGER_TEXT)

    # 4. ëŒ€ë³¸ ì—…ë°ì´íŠ¸
    script_data["script"] = "\n".join(lines)
    
    print(f"  - [ë©˜íŠ¸ ì •ë¦¬] ë¯¸ìŠ¤í„°ë¦¬ ìœ ì§€ (ê°•ì œ ì¶”ì²œ ë©˜íŠ¸ ì‚­ì œ ì™„ë£Œ).")

    return script_data
# ==============================================================================
# --- ğŸ¬ 3ë‹¨ê³„: ë¯¸ë””ì–´ ì—ì…‹ ìƒì„± ëª¨ë“ˆ (ìŒì„±, ì˜ìƒ, íƒ€ì„ë¼ì¸) ---
# ==============================================================================

def _eleven_tts_with_timestamps(text: str, voice_id: str, api_key: str) -> dict | None:
    """
    ElevenLabs API (/v1/text-to-speech/{voice_id}/with-timestamps) í˜¸ì¶œ.
    ì„±ê³µ ì‹œ JSON ì‘ë‹µ(audio_base64, alignment í¬í•¨) ë°˜í™˜.
    """
    if not api_key or not voice_id:
        return None
    try:
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/with-timestamps"
        headers = {
            "xi-api-key": api_key,
            "Content-Type": "application/json",
        }
        # ì„¤ì •ê°’: ì•ˆì •ì„± 0.5, ëª…ë£Œë„ 0.85, ìŠ¤íƒ€ì¼ 0.15 (tet.py ì°¸ì¡°)
        voice_settings = {
            "stability": 0.5,
            "similarity_boost": 0.85,
            "style": 0.15,
            "use_speaker_boost": True,
        }
        payload = {
            "text": text,
            "model_id": ELEVEN_MODEL_ID,
            "voice_settings": voice_settings
        }
        # íƒ€ì„ì•„ì›ƒ ë„‰ë„‰í•˜ê²Œ
        r = requests.post(url, headers=headers, json=payload, timeout=45)
        if r.status_code == 200:
            return r.json()
        print(f"âš ï¸ ElevenLabs API ì˜¤ë¥˜: {r.status_code} / {r.text[:200]}")
        return None
    except Exception as e:
        print(f"âš ï¸ ElevenLabs API ì˜ˆì™¸: {e}")
        return None

def _subtitle_chunks_from_alignment(align: dict, max_chars_per_line: int = 15, max_lines: int = 2) -> list:
    """
    [ìˆ˜ì •ë¨] ë‹¨ì–´ ë‹¨ìœ„ ë³´ì¡´ + 2ì¤„ ìë§‰ ì§€ì›
    ElevenLabs alignment ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìë§‰ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not align:
        return []
    try:
        chars = align.get("characters") or []
        starts = align.get("character_start_times_seconds") or []
        ends = align.get("character_end_times_seconds") or []
        if not chars or not starts or not ends:
            return []

        # 1. ë¬¸ì -> ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¬¶ê¸° (ë‹¨ì–´, ì‹œì‘, ë)
        words = []
        cw = ""
        cs = None 

        for i, c in enumerate(chars):
            if cs is None: cs = float(starts[i])
            if c == " ": 
                if cw.strip():
                    words.append({"text": cw.strip(), "start": cs, "end": float(ends[i-1])})
                cw = ""
                cs = float(starts[i]) if i < len(starts) else float(ends[i-1])
            else:
                cw += c
        if cw.strip():
            words.append({"text": cw.strip(), "start": cs, "end": float(ends[-1])})

        # 2. ë‹¨ì–´ -> ìë§‰ ì²­í¬ (2ì¤„ ë¡œì§)
        chunks = []
        current_chunk_words = []
        current_lines = [""]
        chunk_start = 0.0
        
        for w in words:
            word_txt = w["text"]
            if not current_chunk_words:
                chunk_start = w["start"]
            
            # í˜„ì¬ ì¤„ì— ë‹¨ì–´ë¥¼ ë”í–ˆì„ ë•Œ ê¸¸ì´ ì²´í¬
            test_line = (current_lines[-1] + " " + word_txt).strip()
            
            if len(test_line) <= max_chars_per_line:
                # ê°™ì€ ì¤„ì— ì¶”ê°€
                current_lines[-1] = test_line
                current_chunk_words.append(w)
            else:
                # ì¤„ë°”ê¿ˆ í•„ìš”
                if len(current_lines) < max_lines:
                    # ë‹¤ìŒ ì¤„ë¡œ ë„˜ê¹€
                    current_lines.append(word_txt)
                    current_chunk_words.append(w)
                else:
                    # ì¤„ ê½‰ ì°¸ -> í˜„ì¬ ì²­í¬ ë§ˆê°í•˜ê³  ìƒˆë¡œ ì‹œì‘
                    # ë§ˆê°
                    chunk_end = current_chunk_words[-1]["end"]
                    final_text = "\n".join(current_lines)
                    chunks.append({"text": final_text, "start": chunk_start, "end": chunk_end + 0.1}) # ì—¬ìœ  0.1ì´ˆ
                    
                    # ì´ˆê¸°í™” (í˜„ì¬ ë‹¨ì–´ë¶€í„° ìƒˆ ì²­í¬)
                    current_lines = [word_txt]
                    current_chunk_words = [w]
                    chunk_start = w["start"]

        # ë‚¨ì€ ê²ƒ ì²˜ë¦¬
        if current_chunk_words:
            chunk_end = current_chunk_words[-1]["end"]
            final_text = "\n".join(current_lines)
            chunks.append({"text": final_text, "start": chunk_start, "end": chunk_end + 0.1})

        return chunks
    except Exception as e:
        print(f"âš ï¸ ìë§‰ ì²­í¬ ìƒì„± ì˜¤ë¥˜: {e}")
        return []

def generate_eleven_tts_per_sentence(sentences: list[str], api_key: str):
    """
    ë¬¸ì¥ë³„ë¡œ ElevenLabs TTSë¥¼ ìƒì„±í•˜ê³ ,
    ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ + ìë§‰ íƒ€ì´ë° ì •ë³´(alignment ê¸°ë°˜)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*50 + "\nğŸ§ 3ë‹¨ê³„: ElevenLabs TTS (Timestamp í¬í•¨) ìƒì„±...\n" + "="*50)
    
    if not sentences or not api_key:
        print("âš ï¸ ë¬¸ì¥ì´ ì—†ê±°ë‚˜ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    audio_segments = []
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    temp_dir = os.path.join(OUTPUT_DIR, "tts_parts")
    os.makedirs(temp_dir, exist_ok=True)

    for i, sent in enumerate(sentences):
        clean_text = sent.strip()
        if not clean_text:
            continue
            
        print(f"  - TTS ìƒì„± ì¤‘ [{i+1}/{len(sentences)}]: {clean_text[:20]}...")

        # 1. API í˜¸ì¶œ (ë°œìŒ êµì • ì ìš©)
        # ìë§‰ìš© í…ìŠ¤íŠ¸(clean_text)ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , TTSìš© í…ìŠ¤íŠ¸ë§Œ ë³€ê²½
        tts_text = clean_text.replace("íšŒì¶˜41", "íšŒì¶˜ì‚¬ì‹­ì¼")
        rj = _eleven_tts_with_timestamps(tts_text, ELEVENLABS_VOICE_ID, api_key)
        
        if rj and rj.get("audio_base64"):
            # 2. ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ (mp3)
            file_path = os.path.join(temp_dir, f"tts_{i}.mp3")
            try:
                audio_bytes = base64.b64decode(rj["audio_base64"])
                with open(file_path, "wb") as f:
                    f.write(audio_bytes)
                
                # ê¸¸ì´ ì¸¡ì • (moviepy)
                audioclip = AudioFileClip(file_path)
                duration = audioclip.duration
                audioclip.close()

                # 3. ìë§‰ ë°ì´í„° ìƒì„± (alignment í™œìš©)
                alignment = rj.get("alignment")
                # tet.py ì„¤ì •ì„ ì°¸ê³ í•˜ì—¬ ì ì ˆí•œ ê¸¸ì´ë¡œ ìë§‰ ë¶„í• 
                subtitle_chunks = _subtitle_chunks_from_alignment(alignment, max_chars_per_line=14, max_lines=2)
                
                # âœ… [ì¶”ê°€] ìë§‰ í…ìŠ¤íŠ¸ ì›ìƒë³µêµ¬ (ì‚¬ì‹­ì¼ -> 41)
                for chunk in subtitle_chunks:
                    chunk["text"] = chunk["text"].replace("íšŒì¶˜ì‚¬ì‹­ì¼", "íšŒì¶˜41")
                
                # ë§Œì•½ alignment ì‹¤íŒ¨ë¡œ ìë§‰ì´ ì•ˆ ë‚˜ì˜¤ë©´? -> í†µì§œ ìë§‰ìœ¼ë¡œ fallback
                if not subtitle_chunks:
                     subtitle_chunks = [{"text": clean_text, "start": 0.0, "end": duration}]

                audio_segments.append({
                    "path": file_path,
                    "duration": duration,
                    "text": clean_text,
                    "subtitles": subtitle_chunks # âœ… ì—¬ê¸°ê°€ í•µì‹¬! ì •í™•í•œ íƒ€ì´ë° ì •ë³´
                })

            except Exception as e:
                print(f"  - [ì˜¤ë¥˜] íŒŒì¼ ì €ì¥/ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                audio_segments.append({"path": None, "duration": 0, "text": clean_text, "subtitles": []})
        else:
            print("  - [ì‹¤íŒ¨] ElevenLabs API ì‘ë‹µ ì—†ìŒ")
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ì •ë³´
            audio_segments.append({"path": None, "duration": 0, "text": clean_text, "subtitles": []})

    return audio_segments

def get_audio_duration(file_path: str) -> float:
    try: return MP3(file_path).info.length
    except Exception: return 0.0

def get_video_duration(file_path: str) -> float:
    try:
        clip = VideoFileClip(file_path)
        d = clip.duration
        clip.close()
        return d
    except Exception:
        return 0.0

def generate_pexel_search_keyword(client: OpenAI, sentence: str, theme: str) -> str:
    # [ìµœì¢… ìˆ˜ì •] ë¹„ìœ (Metaphor)ë¥¼ ì‹¤ì œ ìƒí™©(Visual)ìœ¼ë¡œ ë²ˆì—­í•˜ì—¬ ê²€ìƒ‰
    prompt = f"""
    Role: Professional Video Editor.
    Task: Convert the Korean sentence into Pexels search keywords for stock footage.
    
    [CRITICAL RULES]
    1. **Translate Metaphors to Reality:**
       - If sentence says "Stomach is a time bomb", DO NOT search 'bomb'. Search **'stomach pain, worried man'**.
       - If sentence says "Blood is like thick syrup", DO NOT search 'syrup'. Search **'blood vessel, clogged artery'**.
       - If sentence says "Poop bag", search **'hospital patient, sad senior, surgery recovery'**.
    
    2. **Simple & Visual:**
       - Use 2~3 concrete English phrases separated by commas.
       - Focus on the visual action or emotion (e.g., "eating meat, doctor explaining, unhealthy food").

    Theme: {theme}
    Sentence: "{sentence}"
    
    Keywords (English, comma separated):
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}], 
            temperature=0.3
        )
        content = response.choices[0].message.content.strip()
        return content.replace('"', '').replace("'", "").strip()
    except Exception:
        return theme.replace(" ", ", ")
    

def search_and_download_pexel_video(api_key: str, keyword: str, output_filename: str, used_video_ids: set):
    # ê²€ìƒ‰ì–´ í›„ë³´êµ°: [GPTí‚¤ì›Œë“œ, ì‰¼í‘œë¶„ë¦¬ í‚¤ì›Œë“œë“¤..., "health", "nature"]
    search_candidates = [keyword]
    if ',' in keyword:
        search_candidates.extend([k.strip() for k in keyword.split(',') if k.strip()])
    
    # ğŸ”¹ [ìˆ˜ì •] ë¬´ì¡°ê±´ ë‹¤ìš´ë¡œë“œ ì„±ê³µì‹œí‚¤ê¸° ìœ„í•œ ê°•ë ¥í•œ ë°±ì—… í‚¤ì›Œë“œ ì¶”ê°€
    search_candidates.append("calm nature")
    search_candidates.append("abstract background")

    headers = {"Authorization": api_key}

    for current_keyword in search_candidates:
        if not current_keyword: continue
        
        search_url = f"https://api.pexels.com/videos/search?query={quote(current_keyword)}&orientation=portrait&per_page=10"
        
        try:
            response = requests.get(search_url, headers=headers, timeout=10)
            
            # ğŸ”¹ [ìˆ˜ì •] ì™œ ì‹¤íŒ¨í–ˆëŠ”ì§€ ë¡œê·¸ ì¶œë ¥ (401ì´ë©´ í‚¤ ë¬¸ì œ, 429ë©´ ì¿¼í„° ë¬¸ì œ)
            if response.status_code != 200:
                print(f"      - [Pexels ì˜¤ë¥˜] í‚¤ì›Œë“œ '{current_keyword}' ì‹¤íŒ¨. Status: {response.status_code}")
                # 401 Unauthorizedë©´ í‚¤ê°€ í‹€ë¦° ê²ƒì´ë¯€ë¡œ ë” ì‹œë„í•´ë„ ì†Œìš©ì—†ìŒ -> ë°”ë¡œ ì¢…ë£Œ
                if response.status_code == 401:
                    print("      - [ì¹˜ëª…ì ] Pexels API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return None
                continue

            data = response.json()
            videos = data.get('videos', [])
            
            # ì‚¬ìš© ì•ˆ í•œ ë¹„ë””ì˜¤ ì°¾ê¸°
            target_video = None
            for v in videos:
                if v['id'] not in used_video_ids:
                    target_video = v
                    break
            
            # ìƒˆ ë¹„ë””ì˜¤ ì—†ìœ¼ë©´ ì•„ë¬´ê±°ë‚˜ ì¬ì‚¬ìš©
            if not target_video and videos:
                target_video = random.choice(videos)

            if target_video:
                video_files = target_video.get('video_files', [])
                # í™”ì§ˆ: HD (width 1080 ê·¼ì²˜) ìš°ì„ , ì—†ìœ¼ë©´ SD
                # PexelsëŠ” width/height ì •ë³´ë¥¼ ì¤ë‹ˆë‹¤.
                best_link = None
                
                # 1ìˆœìœ„: HDê¸‰ (ë„ˆë¹„ 720~1080 ì‚¬ì´)
                for f in video_files:
                    w = f.get('width', 0)
                    if 700 <= w <= 1200: 
                        best_link = f.get('link')
                        break
                
                # 2ìˆœìœ„: ì•„ë¬´ê±°ë‚˜
                if not best_link and video_files:
                    best_link = video_files[0].get('link')

                if best_link:
                    dl_resp = requests.get(best_link, timeout=20)
                    if dl_resp.status_code == 200:
                        with open(output_filename, 'wb') as f: 
                            f.write(dl_resp.content)
                        used_video_ids.add(target_video['id'])
                        print(f"      - ğŸ¯ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (í‚¤ì›Œë“œ: '{current_keyword}')")
                        return output_filename
                    
        except Exception as e:
            print(f"      - [ì˜¤ë¥˜] Pexels ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸: {e}")
            continue

    print("      - [ìµœì¢… ì‹¤íŒ¨] ëª¨ë“  í‚¤ì›Œë“œ(ë°±ì—… í¬í•¨)ë¡œë„ ì˜ìƒì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return None

def split_script_into_sentences(script_text: str):
    """
    ìŠ¤í¬ë¦½íŠ¸ë¥¼ 'ì™„ì „í•œ ë¬¸ì¥' ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    - ë¨¼ì € ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê³ 
    - ê·¸ë˜ë„ ê¸¸ë©´ ì¢…ê²°ì–´ë¯¸(ë‹¤., ìš”., í•©ë‹ˆë‹¤., ? , !) ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬
    """
    if not script_text:
        return []

    text = script_text.strip()

    # 1) ì¤„ë°”ê¿ˆ ê¸°ì¤€ 1ì°¨ ë¶„ë¦¬
    chunks = [s.strip() for s in text.split('\n') if s.strip()]

    # 2) ê° chunkê°€ ë„ˆë¬´ ê¸¸ë©´ ì¢…ê²°ì–´ë¯¸ ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ ë¶„ë¦¬
    sentences = []
    pattern = r'(.*?(?:ë‹¤\.|ìš”\.|í•©ë‹ˆë‹¤\.|í–ˆìŠµë‹ˆë‹¤\.|ì…ë‹ˆë‹¤\.|ì…ë‹ˆê¹Œ\?|ì£ \.|\.|\?|!))\s*'

    for chunk in chunks:
        if len(chunk) < 25:   # ì§§ìœ¼ë©´ ê·¸ëƒ¥ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¸ì •
            sentences.append(chunk)
            continue

        matches = re.findall(pattern, chunk)
        if matches:
            for m in matches:
                s = m.strip()
                if s:
                    sentences.append(s)
        else:
            sentences.append(chunk)

    # í˜¹ì‹œ ì•„ë¬´ê²ƒë„ ì•ˆ ë‚˜ì™”ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ
    if not sentences:
        sentences = [text]

    return sentences
def split_sentence_into_chunks(sentence: str, max_chars: int = 20):
    """
    [ìˆ˜ì •ë¨] ë¬¸ì¥ ë¶€í˜¸(?, !, .)ë¥¼ ì‚­ì œí•˜ì§€ ì•Šê³  ìœ ì§€í•˜ë©° ìë¥´ê¸°
    """
    if not sentence:
        return []

    text = sentence.strip()

    # 1) ë¬¸ì¥ ë¶€í˜¸ë¥¼ í¬í•¨í•´ì„œ ë¶„ë¦¬ (Capturing Group ì‚¬ìš©)
    # ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”? ë°˜ê°‘ìŠµë‹ˆë‹¤." -> ["ì•ˆë…•í•˜ì„¸ìš”", "?", "ë°˜ê°‘ìŠµë‹ˆë‹¤", ".", ""]
    raw_parts = re.split(r'([,.?!])', text)
    
    # ë¶„ë¦¬ëœ ë¶€í˜¸ë¥¼ ì• ë‹¨ì–´ì— ë‹¤ì‹œ ë¶™ì—¬ì¤Œ
    merged_parts = []
    current_part = ""
    for p in raw_parts:
        p = p.strip()
        if not p: continue
        
        # ë§Œì•½ ë¬¸ì¥ ë¶€í˜¸ë¼ë©´ ì§ì „ ë©ì–´ë¦¬ì— ë¶™ì„
        if p in [',', '.', '?', '!']:
            if merged_parts:
                merged_parts[-1] += p
            else:
                current_part += p # ë¬¸ì¥ ì‹œì‘ë¶€í„° ë¶€í˜¸ì¸ ê²½ìš°(ê±°ì˜ ì—†ìŒ)
        else:
            merged_parts.append(p)

    # 2) ì ‘ì†ì–´ ê¸°ì¤€ ë“± 2ì°¨ ë¶„ë¦¬ ë¡œì§ì€ ìœ ì§€í•˜ë˜, ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì—ˆìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    if not merged_parts:
        return [text]

    final_chunks = []
    
    # 3) ê¸€ì ìˆ˜ ê¸¸ì´ë¡œ ë‹¤ë“¬ê¸°
    for part in merged_parts:
        if len(part.replace(" ", "")) <= max_chars:
            final_chunks.append(part)
        else:
            # ë„ˆë¬´ ê¸¸ë©´ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
            words = part.split()
            current = []
            current_len = 0
            for w in words:
                w_len = len(w)
                if current and (current_len + w_len + 1 > max_chars):
                    final_chunks.append(" ".join(current))
                    current = [w]
                    current_len = w_len
                else:
                    current.append(w)
                    current_len += w_len
            if current:
                final_chunks.append(" ".join(current))

    return final_chunks
def create_media_assets_and_timeline(client: OpenAI, script_data: dict, settings: dict):
    print("\n" + "="*50 + "\nğŸ¬ 3ë‹¨ê³„: ë¯¸ë””ì–´ ì—ì…‹ ìƒì„± (ê°œë³„ ì˜¤ë””ì˜¤ íŒŒì¼ ê¸°ë°˜)...\n" + "="*50)

    narration_script = script_data.get('script', '') or ''
    if not narration_script.strip():
        print("[ì˜¤ë¥˜] script_data['script'] ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    # 1) ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
    sentences = split_script_into_sentences(narration_script)
    if not sentences:
        print("[ì˜¤ë¥˜] ë¬¸ì¥ ë¶„í•  ì‹¤íŒ¨")
        return

    # 2) [êµì²´ë¨] ElevenLabs TTS (Timestamp í¬í•¨)
    if not settings.get('eleven_api_key'):
        print("[ì¹˜ëª…ì  ì˜¤ë¥˜] ElevenLabs API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì • íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    audio_segments = generate_eleven_tts_per_sentence(sentences, settings['eleven_api_key'])
    
    if not audio_segments:
        print("[ì˜¤ë¥˜] TTS ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # 4) ì˜ìƒ í…Œë§ˆ
    video_theme = script_data.get('search_keywords', ["senior health"])[0]

    timeline = []
    used_pexel_ids = set()
    last_video_path = None

    talisman_duration = 0.0
    if os.path.exists(TALISMAN_VIDEO_FILE):
        talisman_duration = get_video_duration(TALISMAN_VIDEO_FILE)

    # 5) íƒ€ì„ë¼ì¸ ì¡°í•© ë£¨í”„
    # audio_segments ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ
    for i, seg_info in enumerate(audio_segments):
        audio_path = seg_info["path"]
        audio_duration = seg_info["duration"]
        sentence_text = seg_info["text"]
        
        # ElevenLabsê°€ ì¤€ ì •í™•í•œ ìë§‰ ì •ë³´ (ì—†ìœ¼ë©´ fallback í†µì§œ ìë§‰ì´ ë“¤ì–´ìˆìŒ)
        subtitle_schedule = seg_info.get("subtitles", [])

        if not audio_path: # ì‹¤íŒ¨í•œ ë¬¸ì¥ ìŠ¤í‚µ
            continue

        print(f"\n[ë¬¸ì¥ {i+1}/{len(audio_segments)}] ì˜ìƒ ë§¤ì¹­ ì¤‘...")

        # íŠ¹ìˆ˜ ë¬¸ì¥(í™©ê¸ˆì—´ì‡ ) ì²´í¬
        is_talisman_sentence = EFFECT_TRIGGER_TEXT in sentence_text

        # ì˜ìƒ ì†ŒìŠ¤ ê²°ì •
        if is_talisman_sentence and talisman_duration > 0:
            base_video_path = TALISMAN_VIDEO_FILE
            final_clip_duration = max(audio_duration, talisman_duration)
        else:
            pexel_keyword = generate_pexel_search_keyword(client, sentence_text, video_theme)
            base_video_path = os.path.join(OUTPUT_DIR, f"video_sentence_{i}.mp4")

            downloaded = search_and_download_pexel_video(
                settings['pexels_api_key'], pexel_keyword, base_video_path, used_pexel_ids
            )
            
            if downloaded:
                base_video_path = downloaded
                last_video_path = base_video_path
            else:
                base_video_path = last_video_path # ì‹¤íŒ¨ì‹œ ì§ì „ ì˜ìƒ ì¬ì‚¬ìš©

            final_clip_duration = audio_duration

        # íƒ€ì„ë¼ì¸ ì¶”ê°€ (audio_startëŠ” ì´ì œ í•„ìš” ì—†ìŒ, í•­ìƒ 0)
        # SFX ê²°ì • (ì²« ë¬¸ì¥ì€ ì œì™¸, 2ë²ˆì§¸ ë¬¸ì¥ë¶€í„° ëœë¤ SFX)
        sfx_file = None
        if i > 0 and not is_talisman_sentence:
            sfx_name = random.choice(["pop_3.mp3", "whoosh_fast.mp3"])
            sfx_path = os.path.join(os.path.dirname(BGM_FILE), sfx_name) # bgm.mp3ì™€ ê°™ì€ í´ë”ì— ìˆë‹¤ê³  ê°€ì •
            if os.path.exists(sfx_path):
                sfx_file = sfx_path

        timeline.append({
            "type": "sentence",
            "audio_path": audio_path,        # â˜… ê°œë³„ íŒŒì¼ ê²½ë¡œ
            "audio_start": 0,                # â˜… íŒŒì¼ ì²˜ìŒë¶€í„° ì¬ìƒ
            "duration": final_clip_duration,
            "video_path": base_video_path,
            "subtitles": subtitle_schedule,  # â˜… ElevenLabs alignment ê¸°ë°˜ ìë§‰
            "is_talisman": is_talisman_sentence,
            "sentence_index": i,
            "text": sentence_text,  # âœ… [ì¶”ê°€] í…ìŠ¤íŠ¸ ë‚´ìš© í¬í•¨ (í™”ì‚´í‘œ ë¡œì§ìš©)
            "sfx_path": sfx_file    # âœ… ì”¬ ì „í™˜ íš¨ê³¼ìŒ ê²½ë¡œ ì¶”ê°€
        })

    # timeline.json ì €ì¥
    timeline_file = os.path.join(OUTPUT_DIR, "timeline.json")
    with open(timeline_file, 'w', encoding='utf-8') as f:
        json.dump(timeline, f, ensure_ascii=False, indent=2)

    print("\nğŸ‰ íƒ€ì„ë¼ì¸ ìƒì„± ì™„ë£Œ (ElevenLabs Timestamp ì ìš©)")
# ======================================================================
# --- ğŸ“º YouTube ì—…ë¡œë“œ ëª¨ë“ˆ ---
# ======================================================================

YOUTUBE_SCOPES = ["https://www.googleapis.com/auth/youtube.upload"]
YOUTUBE_CLIENT_SECRET_FILE = "youtube_client_secret.json"  # êµ¬ê¸€ ì½˜ì†”ì—ì„œ ë°›ì€ OAuth client íŒŒì¼
YOUTUBE_TOKEN_FILE = "youtube_token.pickle"               # í† í° ìºì‹œ

def get_youtube_client():
    """
    ìµœì´ˆ 1ë²ˆì€ ë¸Œë¼ìš°ì €ë¡œ ë¡œê·¸ì¸/OAuth í—ˆìš©í•˜ê³ ,
    ì´í›„ë¶€í„°ëŠ” youtube_token.pickleì„ ì¬ì‚¬ìš©í•´ì„œ ìë™ ë¡œê·¸ì¸.
    """
    creds = None

    # ì´ì „ì— ì €ì¥í•´ë‘” í† í°ì´ ìˆìœ¼ë©´ ë¨¼ì € ì‚¬ìš©
    if os.path.exists(YOUTUBE_TOKEN_FILE):
        with open(YOUTUBE_TOKEN_FILE, "rb") as token:
            creds = pickle.load(token)

    # í† í°ì´ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš°
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            # ë¦¬í”„ë ˆì‹œ í† í°ìœ¼ë¡œ ê°±ì‹ 
            creds.refresh(Request())
        else:
            # ìµœì´ˆ 1íšŒ: OAuth ë™ì˜ ë°›ê¸° (ì½˜ì†”ì— URL ëœ¨ê³ , ì½”ë“œ ë³µë¶™í•˜ëŠ” ë°©ì‹)
            flow = InstalledAppFlow.from_client_secrets_file(
            YOUTUBE_CLIENT_SECRET_FILE,
            YOUTUBE_SCOPES
            )
            creds = flow.run_local_server(port=8080, prompt='consent')


        # ë‹¤ìŒë¶€í„°ëŠ” ìë™ ë¡œê·¸ì¸ë˜ë„ë¡ ì €ì¥
        with open(YOUTUBE_TOKEN_FILE, "wb") as token:
            pickle.dump(creds, token)

    youtube = build("youtube", "v3", credentials=creds)
    return youtube


def upload_video_to_youtube(
    youtube,
    video_path: str,
    thumb_path: str,
    title: str,
    description: str,
    tags=None,
    category_id: str = "27",    # 27 = Education (ê±´ê°•/ì •ë³´ ì±„ë„ì´ë©´ ì´ìª½ì´ ë¬´ë‚œ)
    privacy_status: str = "unlisted",  # í…ŒìŠ¤íŠ¸ëŠ” unlistedë¡œ
):
    """
    ì™„ì„±ëœ ìˆì¸  ì˜ìƒ + ì¸ë„¤ì¼ì„ ìœ íŠœë¸Œì— ì—…ë¡œë“œí•˜ê³  ì¸ë„¤ì¼ê¹Œì§€ ì„¤ì •.
    """
    if tags is None:
        tags = []

    # 1) ë©”íƒ€ë°ì´í„°(ì œëª©/ì„¤ëª…/íƒœê·¸/ê³µê°œë²”ìœ„)
    body = {
        "snippet": {
            "title": title,
            "description": description,
            "tags": tags,
            "categoryId": category_id,
        },
        "status": {
            "privacyStatus": privacy_status,
            "selfDeclaredMadeForKids": False,
        },
    }

    media = MediaFileUpload(video_path, chunksize=-1, resumable=True)

    print("\nğŸ“¤ ìœ íŠœë¸Œ ì—…ë¡œë“œ ì‹œì‘...")
    request = youtube.videos().insert(
        part="snippet,status",
        body=body,
        media_body=media
    )

    response = None
    while response is None:
        status, response = request.next_chunk()
        if status:
            print(f"  - ì—…ë¡œë“œ ì§„í–‰ë¥ : {int(status.progress() * 100)}%")

    video_id = response["id"]
    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! videoId = {video_id}")

    # 2) ì¸ë„¤ì¼ ì„¤ì •
    if thumb_path and os.path.exists(thumb_path):
        try:
            print("ğŸ–¼ ì¸ë„¤ì¼ ì—…ë¡œë“œ ì¤‘...")
            youtube.thumbnails().set(
                videoId=video_id,
                media_body=MediaFileUpload(thumb_path)
            ).execute()
            print("âœ… ì¸ë„¤ì¼ ì„¤ì • ì™„ë£Œ!")
        except Exception as e:
            print("âŒ ì¸ë„¤ì¼ ì—…ë¡œë“œ ì‹¤íŒ¨:", e)


def create_thumbnail_image(script_data: dict, thumb_path: str):
    """
    ì¸ë„¤ì¼ ì´ë¯¸ì§€ ìƒì„±:
    - ë°°ê²½: ê²€ì€ìƒ‰
    - í•­ìƒ youtube_titleì„ ê¸°ì¤€ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    - thumbnail_segmentsê°€ ìˆìœ¼ë©´ ìƒ‰ìƒ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³ ,
      ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì œëª© ì•ˆì— ë“±ì¥í•˜ëŠ” ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬í•´ì„œ í‘œì‹œí•œë‹¤.
    """

    # âœ… í•­ìƒ ìœ íŠœë¸Œ ì œëª©ì„ ì¸ë„¤ì¼ ë¬¸êµ¬ë¡œ ì‚¬ìš©
    text = (script_data.get("youtube_title") or "").strip()

    # GPTê°€ ì¤€ thumbnail_segmentsëŠ” "ìƒ‰ì¹  ì •ë³´"ë¡œë§Œ ì‚¬ìš©
    segments = script_data.get("thumbnail_segments") or []

    # 1ì°¨ ì •ë¦¬: ì´ìƒí•œ ê°’ í•„í„°ë§ + ìƒ‰ìƒ ì •ê·œí™”
    cleaned_segments = []
    for seg in segments:
        if not isinstance(seg, dict):
            continue
        t = (seg.get("text") or "").strip()
        c = (seg.get("color") or "white").strip().lower()
        if not t:
            continue
        if c not in ("red", "yellow", "white"):
            c = "white"
        cleaned_segments.append({"text": t, "color": c})

    segments = cleaned_segments

    # ğŸ”¥ 1) thumbnail_segmentsê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ â†’ ì œëª© ì „ì²´ë¡œ ìë™ ìƒì„±
    if not segments:
        tokens = text.split()

        if len(tokens) <= 2:
            # í•œ ë©ì–´ë¦¬ê±°ë‚˜ ì§§ì„ ë•Œ
            segments = [{"text": text, "color": "yellow"}]
        else:
            # ì•ìª½(ì¡°ê±´) / ë’¤ìª½(ê²°ê³¼) ë‚˜ëˆ„ê¸°
            split_idx = max(2, len(tokens) // 2)
            first_part = " ".join(tokens[:split_idx])
            second_part = " ".join(tokens[split_idx:])

            segments = [
                {"text": first_part, "color": "yellow"},
                {"text": second_part, "color": "red"},
            ]
    else:
        # ğŸ”¥ 2) GPTê°€ ì¤€ segmentsê°€ ìˆì„ ë•Œ:
        #     â†’ segmentsì— í¬í•¨ë˜ì§€ ì•Šì€ ì œëª© ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ whiteë¡œ ì¶”ê°€
        remaining = text
        for seg in segments:
            seg_text = seg["text"]
            remaining = remaining.replace(seg_text, " ", 1)

        remaining = " ".join(remaining.split())  # ê³µë°± ì •ë¦¬

        if remaining:
            segments.append({
                "text": remaining,
                "color": "white"
            })

def build_title_segments(script_data: dict):
    """
    [ìˆ˜ì •ë¨] ì˜ìƒ ìƒë‹¨ ì œëª©ì„ ë¬´ì¡°ê±´ 'youtube_title'ê³¼ ë˜‘ê°™ì´ ë§ì¶”ëŠ” í•¨ìˆ˜.
    - í•´ì‹œíƒœê·¸(#ê±´ê°• ë“±)ëŠ” ì œê±°í•©ë‹ˆë‹¤.
    - ë¬¸ì¥ì„ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì•ë¶€ë¶„ì€ í°ìƒ‰, ë’·ë¶€ë¶„ì€ ë…¸ë€ìƒ‰(ë˜ëŠ” ë¹¨ê°•)ìœ¼ë¡œ ì¹ í•©ë‹ˆë‹¤.
    """
    # 1. ìœ íŠœë¸Œ ì œëª© ê°€ì ¸ì˜¤ê¸°
    full_title = (script_data.get("youtube_title") or "").strip()
    if not full_title:
        full_title = "ì‹œë‹ˆì–´ ê±´ê°• ì •ë³´"

    # 2. í•´ì‹œíƒœê·¸(#...) ì œê±°í•˜ê¸°
    # ì˜ˆ: "íƒœê·¹ê¶Œì˜ ë¹„ë°€ #ê±´ê°•" -> "íƒœê·¹ê¶Œì˜ ë¹„ë°€"
    clean_title = re.sub(r'#\S+', '', full_title).strip()

    # 3. GPTê°€ ì¤€ ì„¸ê·¸ë¨¼íŠ¸(ìƒ‰ìƒ ì •ë³´)ê°€ ìˆë‹¤ë©´ ì°¸ê³ í•˜ë˜, í…ìŠ¤íŠ¸ê°€ ë‹¤ë¥´ë©´ ë¬´ì‹œí•˜ê³  ìë™ ìƒì„±
    #    (ì—¬ê¸°ì„œëŠ” ë¬´ì¡°ê±´ ìë™ ìƒì„± ë°©ì‹ìœ¼ë¡œ í†µì¼í•´ì„œ ë¶ˆì¼ì¹˜ ë°©ì§€)
    
    tokens = clean_title.split()
    segments = []

    if len(tokens) == 1:
        # ë‹¨ì–´ê°€ 1ê°œë©´ í†µì§¸ë¡œ ë…¸ë€ìƒ‰
        segments.append({"text": clean_title, "color": "yellow"})
    else:
        # ë‹¨ì–´ê°€ ì—¬ëŸ¬ ê°œë©´ ë°˜ìœ¼ë¡œ ìª¼ê°œì„œ ìƒ‰ìƒ ë‹¤ë¥´ê²Œ
        mid = len(tokens) // 2
        # ì•ë¶€ë¶„ (í°ìƒ‰)
        part1 = " ".join(tokens[:mid])
        # ë’·ë¶€ë¶„ (ë…¸ë€ìƒ‰ - ê°•ì¡°)
        part2 = " ".join(tokens[mid:])
        
        if part1:
            segments.append({"text": part1, "color": "white"})
        if part2:
            segments.append({"text": part2, "color": "yellow"})

    return clean_title, segments
def make_title_clips(script_data: dict, duration: float):
    """
    ì œëª©ì„ ìƒë‹¨ ë°”(TOP_BAR_HEIGHT) ì•ˆì— ë„£ë˜,
    - ì¢Œìš° 10% ì •ë„ëŠ” í•­ìƒ ë¹„ì›Œë‘ëŠ” 'ì„¸ì´í”„ ì¡´'ìœ¼ë¡œ ì„¤ì •
    - ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ í°íŠ¸ í¬ê¸°ë¥¼ ìë™ìœ¼ë¡œ ì¤„ì—¬ì„œ ì˜ë¦¬ì§€ ì•Šê²Œ ë§Œë“¦
    """
    _, segments = build_title_segments(script_data)

    # âœ… ì œëª© í°íŠ¸ ì„¤ì • (ìë§‰ë³´ë‹¤ ì»¤ì•¼ í•¨, ìë§‰=85)
    BASE_FONTSIZE = 120         # ê¸°ë³¸ í¬ê¸° ëŒ€í­ í™•ëŒ€
    MIN_FONTSIZE = 80           # ìµœì†Œ í¬ê¸°ë„ ìë§‰ê³¼ ë¹„ìŠ·í•˜ê²Œ ìœ ì§€
    SAFE_RATIO = 0.9            # í™”ë©´ ê°€ë¡œ ê½‰ ì°¨ê²Œ (ì—¬ë°± ì¤„ì„)
    safe_width = SHORTS_WIDTH * SAFE_RATIO

    def build_lines(fontsize: int):
        """ì£¼ì–´ì§„ fontsizeë¡œ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ TextClipìœ¼ë¡œ ë§Œë“¤ê³  ì¤„ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ ë°˜í™˜"""
        seg_clips = []
        for seg in segments:
            seg_text = seg["text"]
            color_tag = seg["color"]
            color_val = "yellow" if color_tag == "yellow" else ("#ff0000" if color_tag == "red" else "white")

            try:
                clip = TextClip(
                    seg_text,
                    fontsize=fontsize,
                    color=color_val,
                    font=FONT_FILE,
                    stroke_color='black',
                    stroke_width=6, # í…Œë‘ë¦¬ë„ ë‘ê»ê²Œ
                    method='label'
                ).set_duration(duration)
                seg_clips.append(clip)
            except Exception as e:
                print("[ì œëª©] ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì˜¤ë¥˜:", e)

        if not seg_clips:
            return [], 0

        # ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ëˆ„ê¸° (í•œ ì¤„ì´ safe_widthë¥¼ ë„˜ì§€ ì•Šë„ë¡)
        lines = []
        current_line = []
        current_w = 0
        max_line_w = 0

        for clip in seg_clips:
            if current_line and current_w + clip.w > safe_width:
                lines.append(current_line)
                max_line_w = max(max_line_w, current_w)
                current_line = [clip]
                current_w = clip.w
            else:
                current_line.append(clip)
                current_w += clip.w

        if current_line:
            lines.append(current_line)
            max_line_w = max(max_line_w, current_w)

        return lines, max_line_w

    # 1ì°¨: ê¸°ë³¸ í°íŠ¸ í¬ê¸°ë¡œ ì¤„ êµ¬ì„±
    lines, max_line_w = build_lines(BASE_FONTSIZE)

    # ë§Œì•½ ê°€ì¥ ê¸´ ì¤„ì´ safe_widthë¥¼ ë„˜ìœ¼ë©´ â†’ í°íŠ¸ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ë‹¤ì‹œ ë¹Œë“œ
    if max_line_w > safe_width and max_line_w > 0:
        scale = safe_width / max_line_w
        new_fontsize = max(MIN_FONTSIZE, int(BASE_FONTSIZE * scale))
        print(f"[ì œëª©] í°íŠ¸ í¬ê¸° ìë™ ì¡°ì •: {BASE_FONTSIZE} â†’ {new_fontsize}")
        lines, max_line_w = build_lines(new_fontsize)

    if not lines:
        return []

    # ì „ì²´ ë†’ì´ ê³„ì‚°
    total_h = sum(max(c.h for c in line) for line in lines) + 15 * (len(lines) - 1)

    # ì œëª©ì€ ìƒë‹¨ ë°” ì•ˆì—ì„œ 'ì•„ë˜ìª½'ì— ë¶™ì´ê¸° (ì˜ìƒì´ë‘ ê°€ê¹ê²Œ)
    y_start = TOP_BAR_HEIGHT - total_h - 20
    title_clips = []

    y = y_start
    for line in lines:
        line_w = sum(c.w for c in line)
        x_start = (SHORTS_WIDTH - line_w) / 2  # í•­ìƒ ê°€ìš´ë° ì •ë ¬
        x = x_start
        line_h = max(c.h for c in line)

        for clip in line:
            title_clips.append(clip.set_position((x, y)))
            x += clip.w

        y += line_h + 15

    return title_clips

import re

# í•œêµ­ì–´ì—ì„œ ê±°ì˜ ì˜ë¯¸ ì—†ëŠ” ì¡°ì‚¬/ì§§ì€ ë‹¨ì–´ë“¤
KOREAN_STOPWORDS = {
    "ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì—ì„œ","ì—ê²Œ","í•œí…Œ","ë³´ë‹¤","ì²˜ëŸ¼","ê¹Œì§€","ë„",
    "ë§Œ","ì™€","ê³¼","ë‘","í•˜ê³ ","ìœ¼ë¡œ","ë¡œ","ê»˜","ì´ë‚˜","ë‚˜","ë˜","ë˜ê°€","ê²Œ","ë“¯",
    "ê·¸ë¦¬ê³ ","ê·¸ë˜ì„œ","í•˜ì§€ë§Œ","ê·¸ëŸ¬ë‚˜","ë˜","ë˜ëŠ”","í˜¹ì‹œ","ê·¸ë˜ë„","ê·¸ëƒ¥",
}

def _normalize_text(text: str) -> str:
    # ê´„í˜¸, ì‰¼í‘œ ê°™ì€ ê²ƒ ì œê±°
    text = re.sub(r"[\(\)\[\]\"â€œâ€'â€™,.?!â€¦]", " ", text)
    # ì—°ì† ê³µë°± ì •ë¦¬
    return re.sub(r"\s+", " ", text).strip()

def _pick_core_token(tokens: list[str]) -> str:
    if not tokens:
        return ""

    # 1ìˆœìœ„: ìˆ«ì/í¼ì„¼íŠ¸ í¬í•¨ëœ í† í° (ì˜ˆ: "30%", "3ë°°", "100g")
    numeric_tokens = [t for t in tokens if re.search(r"\d", t)]
    if numeric_tokens:
        # ê¸¸ì´ ê¸´ ìˆœìœ¼ë¡œ
        numeric_tokens.sort(key=len, reverse=True)
        return numeric_tokens[0]

    # 2ìˆœìœ„: ê¸¸ì´ 3ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë“¤
    long_tokens = [t for t in tokens if len(t) >= 3]
    if long_tokens:
        long_tokens.sort(key=len, reverse=True)
        return long_tokens[0]

    # 3ìˆœìœ„: ë‚¨ì€ ê²ƒ ì¤‘ ê°€ì¥ ê¸´ ê²ƒ
    tokens.sort(key=len, reverse=True)
    return tokens[0]


def extract_core_keyword(line: str) -> str:
    """
    í•œ ë¬¸ì¥ì—ì„œ 'ê°•ì¡°ìš© í•µì‹¬ ë‹¨ì–´'ë¥¼ 1ê°œ ë½‘ëŠ”ë‹¤.
    - ìˆ«ì/í¼ì„¼íŠ¸/ë‹¨ìœ„ ë“¤ì–´ê°„ ë‹¨ì–´ ìš°ì„ 
    - ê·¸ ë‹¤ìŒ 3ê¸€ì ì´ìƒ ë‹¨ì–´
    - ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì œì¼ ê¸´ ë‹¨ì–´
    """
    if not line:
        return ""

    norm = _normalize_text(line)
    if not norm:
        return ""

    raw_tokens = norm.split(" ")
    # ì¡°ì‚¬/ë¶ˆìš©ì–´ ì œê±° + 1ê¸€ìì§œë¦¬ ì œê±°
    tokens: list[str] = []
    for tok in raw_tokens:
        tok = tok.strip()
        if not tok:
            continue
        if tok in KOREAN_STOPWORDS:
            continue
        if len(tok) == 1:
            continue
        tokens.append(tok)

    if not tokens:
        # ê·¸ë˜ë„ í•˜ë‚˜ëŠ” ì£¼ì: ì›ë˜ ë¬¸ì¥ì—ì„œ ì œì¼ ê¸´ í† í°
        tmp = norm.split(" ")
        tmp = [t for t in tmp if t.strip()]
        if not tmp:
            return ""
        tmp.sort(key=len, reverse=True)
        return tmp[0]

    return _pick_core_token(tokens)

def build_sentence_keyword_map(script_data: dict) -> list[str]:
    """
    script_data['script']ì˜ ê° ì¤„(ë¬¸ì¥)ë§ˆë‹¤
    ê°•ì¡°í•  í•µì‹¬ ë‹¨ì–´ë¥¼ 1ê°œì”© ë½‘ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    """
    script_text = (script_data.get("script") or "").strip()
    if not script_text:
        return []

    # [ìˆ˜ì •] ë‹¨ìˆœ split('\n') ëŒ€ì‹ , íƒ€ì„ë¼ì¸ ìƒì„± ë•Œì™€ ë™ì¼í•œ í•¨ìˆ˜ ì‚¬ìš©
    lines = split_script_into_sentences(script_text)

    # 1) GPTê°€ ë¯¸ë¦¬ highlight_keywords ë§Œë“¤ì–´ ì¤¬ìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©
    custom_keywords = script_data.get("highlight_keywords")
    # ì£¼ì˜: split ë°©ì‹ì´ ë‹¬ë¼ì§€ë©´ ê°œìˆ˜ê°€ ì•ˆ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°œìˆ˜ê°€ ì¼ì¹˜í•  ë•Œë§Œ ì‚¬ìš©
    if isinstance(custom_keywords, list) and len(custom_keywords) == len(lines):
        print("\n[ğŸ” highlight_keywords ì‚¬ìš©]")
        for i, (line, kw) in enumerate(zip(lines, custom_keywords)):
            print(f"  - ë¬¸ì¥{i+1}: í‚¤ì›Œë“œ '{kw}' (ë³¸ë¬¸: {line[:10]}...)")
        return custom_keywords

    # 2) ì•„ë‹ˆë©´ ìš°ë¦¬ê°€ ì§ì ‘ ë½‘ê¸°
    keywords: list[str] = []
    print("\n[ğŸ” build_sentence_keyword_map] ë¬¸ì¥ë³„ í•µì‹¬ë‹¨ì–´ ì¶”ì¶œ:")
    for i, line in enumerate(lines):
        kw = extract_core_keyword(line)
        keywords.append(kw)
        # ë¡œê·¸ í™•ì¸ìš©
        print(f"  - ë¬¸ì¥{i+1}: í‚¤ì›Œë“œ '{kw}' (ë³¸ë¬¸: {line[:10]}...)")

    return keywords

# ==============================================================================
# --- ğŸš€ 4ë‹¨ê³„: ìµœì¢… ì˜ìƒ í•©ì„± ëª¨ë“ˆ ---
# ==============================================================================
EFFECT_SOUND_FILE2 = "countdown.mp3" 
def create_video_from_timeline(output_filename, script_data):
    print("\n" + "="*50 + "\nğŸš€ 4ë‹¨ê³„: ìµœì¢… ì˜ìƒ í•©ì„± (ê°œë³„ íŒŒì¼ ë°©ì‹)...\n" + "="*50)

    sentence_keywords = build_sentence_keyword_map(script_data)
    timeline_file = os.path.join(OUTPUT_DIR, "timeline.json")
    
    try:
        with open(timeline_file, 'r', encoding='utf-8') as f:
            timeline_data = json.load(f)
    except Exception:
        return

    final_clips = []

    for i, item in enumerate(timeline_data):
        print(f"    - ë¬¸ì¥ í´ë¦½ [{i+1}/{len(timeline_data)}] í•©ì„± ì¤‘...")

        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ (subclip ì‚¬ìš© ì•ˆ í•¨)
        audio_path = item.get("audio_path")
        if not audio_path or not os.path.exists(audio_path):
            continue

        try:
            # â˜… ìˆ˜ì •ë¨: íŒŒì¼ ìì²´ë¥¼ ë¡œë“œí•˜ë¯€ë¡œ subclip í•„ìš” ì—†ìŒ
            tts_clip = AudioFileClip(audio_path).volumex(1.25)
            
            # ì”¬ ì „í™˜ íš¨ê³¼ìŒ(SFX) í•©ì„±
            sfx_path = item.get("sfx_path")
            if sfx_path and os.path.exists(sfx_path):
                sfx_clip = AudioFileClip(sfx_path).volumex(0.4) # íš¨ê³¼ìŒ ë³¼ë¥¨ ì¡°ì ˆ
                # TTSì™€ SFX í•©ì„± (SFXëŠ” 0ì´ˆë¶€í„° ì‹œì‘)
                audio_clip = CompositeAudioClip([tts_clip, sfx_clip])
            else:
                audio_clip = tts_clip
                
        except Exception as e:
            print(f"      [ì˜¤ë¥˜] ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

        # durationì€ ì˜¤ë””ì˜¤ ê¸¸ì´ë¡œ ê³ ì •
        duration = float(audio_clip.duration)

        # 2. ë¹„ë””ì˜¤ ë¡œë“œ ë° ë£¨í”„/í¬ë¡­
        # ğŸš¨ [ì‚¬ìš©ì ìš”ì²­] 'ì´ê²ƒ', 'ì£¼ì˜' ë“± ë¹¨ê°„ ë§› í‚¤ì›Œë“œê°€ ìˆëŠ” ë¬¸ì¥ì—ì„œëŠ” 
        #    ì˜ìƒì„ ë„ê³ (ê²€ì€ í™”ë©´) ìë§‰ì—ë§Œ ì§‘ì¤‘í•˜ê²Œ í•´ì„œ ë¯¸ìŠ¤í„°ë¦¬/ê³µí¬ê°ì„ ì¡°ì„±í•¨
        RED_KEYWORDS_FOR_BLACK_SCREEN = ["ì´ê²ƒ", "ì´ ì„±ë¶„", "ì£¼ì˜", "ê²½ê³ ", "ì ˆëŒ€", "ìœ„í—˜", "ë…", "ê°€ì§œ", "ë¶€ì‘ìš©", "ë¬¸ì œ", "ì¶©ê²©"]
        should_show_black_screen = any(k in item.get("text", "") for k in RED_KEYWORDS_FOR_BLACK_SCREEN)

        video_path = item.get("video_path")
        
        if should_show_black_screen:
            # ê²€ì€ í™”ë©´ (ë¹„ë””ì˜¤ ë¡œë“œ ìƒëµ)
            print(f"      [ì—°ì¶œ] '{item.get('text', '')[:10]}...' -> ğŸŒ‘ ê²€ì€ í™”ë©´ ì²˜ë¦¬ (ë¯¸ìŠ¤í„°ë¦¬ íš¨ê³¼)")
            vid = ColorClip(size=(SHORTS_WIDTH, VIDEO_AREA_HEIGHT), color=(0,0,0), duration=duration)
        elif video_path and os.path.exists(video_path):
            try:
                vid = VideoFileClip(video_path)
                # ì˜ìƒì´ ì˜¤ë””ì˜¤ë³´ë‹¤ ì§§ìœ¼ë©´ ë£¨í”„
                if vid.duration < duration:
                    vid = vid.loop(duration=duration)
                else:
                    vid = vid.subclip(0, duration)
                
                # ë¹„ìœ¨ ë§ì¶”ê¸° (9:16)
                target_ratio = SHORTS_WIDTH / VIDEO_AREA_HEIGHT
                vid_ratio = vid.w / vid.h
                if vid_ratio > target_ratio:
                    vid = vid.resize(height=VIDEO_AREA_HEIGHT)
                    vid = vid.crop(x_center=vid.w / 2, width=SHORTS_WIDTH)
                else:
                    vid = vid.resize(width=SHORTS_WIDTH)
                    if vid.h < VIDEO_AREA_HEIGHT:
                        vid = vid.resize(height=VIDEO_AREA_HEIGHT)
                    vid = vid.crop(y_center=vid.h / 2, height=VIDEO_AREA_HEIGHT)
                
                # âœ… [ì‹œì²­ì§€ì†ìœ¨ UP] ë‹¤ì–‘í•œ ì¼„ ë²ˆìŠ¤ íš¨ê³¼ (Ken Burns Effect) ëœë¤ ì ìš©
                # 4070 ì„¸ëŒ€ê°€ ì–´ì§€ëŸ½ì§€ ì•Šê²Œ ë¶€ë“œëŸ¬ìš´ ì›€ì§ì„ë§Œ ì‚¬ìš© (Zoom In/Out)
                # Pan íš¨ê³¼ëŠ” ìì¹« ê²€ì€ í™”ë©´(Out of bounds)ì´ ë‚˜ì˜¬ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì œê±°
                effects = ['zoom_in', 'zoom_out']
                effect_type = random.choice(effects)

                if effect_type == 'zoom_in':
                    # ì²œì²œíˆ í™•ëŒ€ (1.0 -> 1.05)
                    vid = vid.resize(lambda t: 1 + 0.04 * t)
                elif effect_type == 'zoom_out':
                    # ì²œì²œíˆ ì¶•ì†Œ (1.05 -> 1.0)
                    vid = vid.resize(lambda t: 1.05 - 0.04 * t)
                else: 
                    # ê¸°ë³¸: ì¤Œì¸ (ê°€ì¥ ë¬´ë‚œ)
                    vid = vid.resize(lambda t: 1 + 0.04 * t)

            except:
                vid = ColorClip(size=(SHORTS_WIDTH, VIDEO_AREA_HEIGHT), color=(0,0,0), duration=duration)
        else:
            vid = ColorClip(size=(SHORTS_WIDTH, VIDEO_AREA_HEIGHT), color=(0,0,0), duration=duration)

        vid = vid.set_position(("center", TOP_BAR_HEIGHT))

        # 3. ë°°ê²½
        bg = ColorClip(size=(SHORTS_WIDTH, SHORTS_HEIGHT), color=(0,0,0), duration=duration)

        # 4. ìë§‰ ìƒì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        subtitle_clips = []
        sub_list = item.get("subtitles", [])
        
        # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ ì¤€ë¹„
        s_idx = item.get("sentence_index", 0)
        highlight_word = ""
        if 0 <= s_idx < len(sentence_keywords):
            highlight_word = sentence_keywords[s_idx]
        h_split = highlight_word.split()
        highlight_tokens = [h for h in h_split if len(h) >= 2]

        subtitle_y = SHORTS_HEIGHT - BOTTOM_BAR_HEIGHT + 50
        max_line_width = int(SHORTS_WIDTH * 0.9)
        txt_kwargs = {'font': FONT_FILE, 'fontsize': 85, 'stroke_color': 'black', 'stroke_width': 4}

        # ìë§‰ ìƒ‰ìƒ ê²°ì • ë¡œì§ (í‚¤ì›Œë“œ ê¸°ë°˜)
        RED_KEYWORDS = ["ì´ê²ƒ", "ì´ ì„±ë¶„", "ì£¼ì˜", "ê²½ê³ ", "ì ˆëŒ€", "ìœ„í—˜", "ë…", "ê°€ì§œ", "ë¶€ì‘ìš©", "ë¬¸ì œ"]
        YELLOW_KEYWORDS = ["íš¨ê³¼", "í•´ê²°", "ì •ë‹µ", "ì¶”ì²œ", "í•µì‹¬", "ê¸°ì¤€", "ìµœê³ ", "ë¹„ë²•", "ë¹„ê²°", "ì¤‘ìš”", "í™•ì¸"]

        for sub in sub_list:
            txt_str = sub["text"]
            start_t = float(sub["start"])
            dur = float(sub.get("duration", float(sub["end"]) - start_t))
            
            # ìƒ‰ìƒ íŒë³„
            sub_color = 'white'
            if any(k in txt_str for k in RED_KEYWORDS):
                sub_color = '#ff0000' # ë¹¨ê°• (ê¸´ì¥/ë¯¸ìŠ¤í„°ë¦¬)
            elif any(k in txt_str for k in YELLOW_KEYWORDS):
                sub_color = 'yellow'  # ë…¸ë‘ (ê°•ì¡°/ê¸ì •)
            
            # í°íŠ¸ ì™¸ê³½ì„ (Stroke)ì€ ê²€ì€ìƒ‰ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ê°€ë…ì„± í™•ë³´
            txt_clip = (TextClip(txt_str, font=FONT_FILE, fontsize=85, color=sub_color, 
                                 stroke_color='black', stroke_width=4,
                                 size=(int(SHORTS_WIDTH*0.9), None), method='caption', align='center')
                        .set_start(start_t).set_duration(dur).set_position(("center", subtitle_y)))
            subtitle_clips.append(txt_clip)

        # 5. ì œëª© í´ë¦½
        title_clips = make_title_clips(script_data, duration)

        # â˜… 6. í™”ì‚´í‘œ ì˜¤ë²„ë ˆì´ (êµ¬ë§¤ ìœ ë„ìš©)
        # ëŒ€ë³¸ì— "ì±„ë„ í™ˆ", "ë§í¬", "í™•ì¸" ê°™ì€ ë‹¨ì–´ê°€ ë“¤ë¦¬ë©´
        # ì™¼ìª½ í•˜ë‹¨(ì±„ë„ ì•„ì´ì½˜/ë§í¬ ìœ„ì¹˜)ì„ ê°€ë¦¬í‚¤ëŠ” í™”ì‚´í‘œë¥¼ ë„ì›€
        arrow_clip = None
        check_text = item.get("text", "")
        if "ì±„ë„ í™ˆ" in check_text or "ë§í¬" in check_text or "ì •ë³´" in check_text:
             # âœ… [í´ë¦­ë¥  UP] ë‘¥ë‘¥ ë– ë‹¤ë‹ˆëŠ” ì• ë‹ˆë©”ì´ì…˜ (Bobbing)
             # x=180 ê³ ì •, yëŠ” 1620ì„ ê¸°ì¤€ìœ¼ë¡œ ìœ„ì•„ë˜ë¡œ 15í”½ì…€ì”© ì›€ì§ì„
             import math
             def bobbing_pos(t):
                 return (180, 1620 + 15 * math.sin(5 * t))

             arrow_clip = (TextClip("â†™ï¸ ì œí’ˆ ì •ë³´", font=FONT_FILE, fontsize=90, color='yellow', stroke_color='black', stroke_width=4)
                           .set_position(bobbing_pos) 
                           .set_duration(duration)
                           .crossfadein(0.5))

        # 7. í•©ì„±
        clips_to_compose = [bg, vid] + title_clips + subtitle_clips
        if arrow_clip:
            clips_to_compose.append(arrow_clip)

        combined = CompositeVideoClip(clips_to_compose)
        combined = combined.set_audio(audio_clip).set_duration(duration)
        final_clips.append(combined)

    if final_clips:
        full_video = concatenate_videoclips(final_clips, method="compose")
        
        # âœ… ì „ì²´ ì†ë„ ì—… (1.15ë°°ë¡œ ë” ë¹ ë¥´ê²Œ!)
        try:
            full_video = full_video.fx(vfx.speedx, 1.15)
        except Exception as e:
            print(f"âš ï¸ speedx ì ìš© ì‹¤íŒ¨(ë¬´ì‹œ ê°€ëŠ¥): {e}")
        
        # BGM ì¶”ê°€
        audio_tracks = [full_video.audio]
        if os.path.exists(BGM_FILE):
            bgm = AudioFileClip(BGM_FILE).set_duration(full_video.duration).volumex(0.4)
            audio_tracks.append(bgm)
        if os.path.exists(INTRO_SOUND_FILE):
            intro = AudioFileClip(INTRO_SOUND_FILE).volumex(1.0).set_start(0)
            audio_tracks.append(intro)
            
        full_video = full_video.set_audio(CompositeAudioClip(audio_tracks))

        full_video.write_videofile(
            output_filename,
            fps=30,
            codec="libx264",
            audio_codec="aac",
            preset="medium",
            ffmpeg_params=["-pix_fmt", "yuv420p"],
            threads=4,
        )
    else:
        print("âš ï¸ í•©ì„±í•  í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")


# ==============================================================================
# --- âš¡ï¸ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ---
# ==============================================================================
VIDEOS_PER_RUN = 3  # í•œ ë²ˆ ì‹¤í–‰í•  ë•Œ ë§Œë“¤ ì˜ìƒ ê°œìˆ˜ (ì›í•˜ë©´ 2, 4 ì´ëŸ° ì‹ìœ¼ë¡œ ë°”ê¿”ë„ ë¨)

if __name__ == "__main__":
    settings = load_settings()
    if not settings:
        print("\n[ì‘ì—… ì¤‘ë‹¨] ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
    else:
        client = OpenAI(api_key=settings['openai_api_key'])

        # âœ… 1) ì´ì „ì— ì‚¬ìš©í•œ ê¸°ì‚¬ URL ë¶ˆëŸ¬ì˜¤ê¸°
        used_urls = load_used_articles()

        # ğŸ“º ìœ íŠœë¸Œ í´ë¼ì´ì–¸íŠ¸ëŠ” í•œ ë²ˆë§Œ ë§Œë“¤ê³  ì¬ì‚¬ìš©
        youtube = None
        try:
            youtube = get_youtube_client()
        except Exception as e:
            print("\n[ê²½ê³ ] ìœ íŠœë¸Œ ì¸ì¦ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì´ë²ˆ ì‹¤í–‰ì—ì„œëŠ” ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            print(e)

        made_count = 0

        # ğŸ” ì—¬ê¸°ì„œë¶€í„° ìµœëŒ€ VIDEOS_PER_RUNê°œê¹Œì§€ ë°˜ë³µ ìƒì„±
        for idx in range(1, VIDEOS_PER_RUN + 1):
            print("\n" + "=" * 50)
            print(f"ğŸ¬ {idx}ë²ˆì§¸ ì˜ìƒ ì œì‘ ì‹œì‘")
            print("=" * 50)

            # ğŸ”¹ [ìˆ˜ì •] ìŠ¤í¬ë˜í•‘ + ìˆ˜ìµì„± í‰ê°€ + ì¤‘ë³µ ì²´í¬ë¥¼ í•œ ë²ˆì—! (Early Exit)
            target_articles_list = find_one_profitable_article(client, used_urls)

            if not target_articles_list:
                print("  - [ì¢…ë£Œ] ë” ì´ìƒ ì œì‘ ê°€ëŠ¥í•œ ê¸°ì‚¬(ë¯¸ì‚¬ìš© + ìˆ˜ìµì„±O)ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            target_article = target_articles_list[0]
            print(f"  - [ì„ íƒ ì™„ë£Œ] '{target_article['title']}' ê¸°ì‚¬ë¡œ ì œì‘í•©ë‹ˆë‹¤.")

            script_data = generate_script_for_seniors(
                client,
                target_article['title'],
                target_article['content']
            )
            if not script_data:
                print("  - [ì‘ì—… ì¤‘ë‹¨] ëŒ€ë³¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´ íšŒì°¨ëŠ” ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue

            yt_title = script_data.get("youtube_title") or "ì‹œë‹ˆì–´ ê±´ê°• ì •ë³´"

            # ğŸ”¹ ì œëª© ê¸°ë°˜ íŒŒì¼ëª… + ì¸ë±ìŠ¤ ë²ˆí˜¸ê¹Œì§€ ë¶™ì—¬ì„œ ì¤‘ë³µ ë°©ì§€
            base_name_with_ext = build_output_filename_from_title(yt_title)
            base_name, ext = os.path.splitext(base_name_with_ext)
            output_filename = f"{base_name}_{idx}{ext}"

            print(f"  - ìµœì¢… ì˜ìƒ íŒŒì¼ëª…: {output_filename}")

                # ì¸ë„¤ì¼ íŒŒì¼ë„ ì¸ë±ìŠ¤ í¬í•¨
            thumb_filename = os.path.splitext(output_filename)[0] + "_thumb.png"
            thumb_path = os.path.join(OUTPUT_DIR, thumb_filename)
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            create_thumbnail_image(script_data, thumb_path)

            # ì¿ íŒ¡ + ë§í¬íŠ¸ë¦¬ ì—°ë™ (í†µí•©ëœ script_data í™œìš©)
            product_info = add_related_coupang_product(client, script_data)
            selected_product = product_info["product"] if product_info else None

            # âœ… ë§ˆì§€ë§‰ í•œ ë¬¸ì¥ì— í´ë¦­ ìœ ë„ ë©˜íŠ¸ ì‚½ì… (ê°€ì„±ë¹„ ëª¨ë“œ)
            script_data = apply_final_closing_line(script_data, selected_product)

            # ---------------------------------------------------------
            # ğŸ“œ [ì¶”ê°€ëœ ì½”ë“œ] ìµœì¢… ëŒ€ë³¸ ë¡œê·¸ ì¶œë ¥
            # ---------------------------------------------------------
            print("\n" + "="*20 + " [ìµœì¢… í™•ì • ëŒ€ë³¸] " + "="*20)
            print(f"ğŸ“º ì œëª©: {script_data.get('youtube_title', 'ì œëª© ì—†ìŒ')}")
            print("-" * 56)
            print(script_data.get('script', 'ëŒ€ë³¸ ì—†ìŒ'))
            print("-" * 56)
            # ---------------------------------------------------------

            if TEST_MODE_SCRIPT_ONLY:
                print("\n[ì•Œë¦¼] ëŒ€ë³¸ ìƒì„± í…ŒìŠ¤íŠ¸ ëª¨ë“œì…ë‹ˆë‹¤. ì˜ìƒ ì œì‘ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                # í…ŒìŠ¤íŠ¸ë¼ë„ ì´ë²ˆ ëŸ°íƒ€ì„ ë‚´ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ë©”ëª¨ë¦¬ì—ë§Œ ê¸°ë¡
                used_urls.add(target_article["url"])
                continue

            # íƒ€ì„ë¼ì¸ ìƒì„± + ì˜ìƒ í•©ì„±
            create_media_assets_and_timeline(client, script_data, settings)
            create_video_from_timeline(output_filename, script_data)
            
            # âœ… ì—…ë¡œë“œ ì‹œë„
            try:
                if ENABLE_YOUTUBE_UPLOAD and youtube is not None:
                    video_path = os.path.abspath(output_filename)
                    thumb_abs_path = os.path.abspath(thumb_path)

                    description = ""  # í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— ì±„ì›Œ ë„£ê¸°
                    tags = script_data.get("search_keywords") or []

                    upload_video_to_youtube(
                        youtube=youtube,
                        video_path=video_path,
                        thumb_path=thumb_abs_path,
                        title=yt_title,
                        description=description,
                        tags=tags,
                        privacy_status="public",  # ì‹¤ì œ ê³µê°œ
                    )
                elif not ENABLE_YOUTUBE_UPLOAD:
                    print("  - [ì•Œë¦¼] ENABLE_YOUTUBE_UPLOADê°€ Falseì´ë¯€ë¡œ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                else:
                    print("  - [ì•Œë¦¼] ìœ íŠœë¸Œ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì—…ë¡œë“œëŠ” ìƒëµí•©ë‹ˆë‹¤.")

                # âœ… ì—…ë¡œë“œê¹Œì§€(ë˜ëŠ” ìµœì†Œí•œ ì˜ìƒ ìƒì„±ê¹Œì§€) ì„±ê³µí–ˆìœ¼ë©´, ì‚¬ìš©í•œ ê¸°ì‚¬ URL ê¸°ë¡
                used_urls.add(target_article["url"])
                save_used_articles(used_urls)
                made_count += 1

            except Exception as e:
                print("\n[ê²½ê³ ] ìœ íŠœë¸Œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ë¡œì»¬ íŒŒì¼ê¹Œì§€ë§Œ ìƒì„±í•©ë‹ˆë‹¤.")
                print(e)
                # ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ used_urlsì— ì•ˆ ë„£ê³ , ë‹¤ìŒ ì‹¤í–‰ì—ì„œ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆê²Œ ë†”ë‘ 
            
        print(f"\nâœ… ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì´ {made_count}ê°œ ì˜ìƒ ì œì‘ ì™„ë£Œ")
